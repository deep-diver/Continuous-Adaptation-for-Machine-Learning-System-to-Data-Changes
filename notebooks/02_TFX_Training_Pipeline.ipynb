{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom_Model_TFX",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/02_TFX_Training_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVp-9PGYFIO"
      },
      "source": [
        "This notebook assumes you are familiar with the basics of Vertex AI, TFX (especially custom components), and TensorFlow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7gJqmqrsfqh"
      },
      "source": [
        "## References\n",
        "\n",
        "This notebook refers to the following resources and also reuses parts of the code from there: \n",
        "* [Simple TFX Pipeline for Vertex Pipelines](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/gcp/vertex_pipelines_simple.ipynb)\n",
        "* [Vertex AI Training with TFX and Vertex Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training)\n",
        "* [Importing models to Vertex AI](https://cloud.google.com/vertex-ai/docs/general/import-model)\n",
        "* [Deploying a model using the Vertex AI API](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)\n",
        "* [MLOPs with Vertex AI](https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai)\n",
        "* [Custom components TFX](https://www.tensorflow.org/tfx/tutorials/tfx/python_function_component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9aBRdubPFsU"
      },
      "source": [
        "## Prerequisites\n",
        "- Enable Vertex AI API\n",
        "- Add the following rules to IAM\n",
        "  - Vertex AI Custom Code Service Agent\n",
        "  - Vertex AI Service Agent\n",
        "  - Vertex AI User\n",
        "  - Artifact Registry Service Agent\n",
        "  - Container Registry Service Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D04aKMGWXjOu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_niUhp_TY1G"
      },
      "source": [
        "# Use the latest version of pip.\n",
        "%%capture\n",
        "!pip install --upgrade tfx==1.2.0 kfp==1.6.1\n",
        "!pip install -q --upgrade google-cloud-aiplatform"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVmgQ6w1oT_Z"
      },
      "source": [
        "### ***Please restart runtime before continuing.*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mstgsNHWoiXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26cf5044-1a73-4ee1-9a7d-d6fcffaee4ba"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [gde] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  region: us-central1\n",
            "  zone: us-central1-a\n",
            "core:\n",
            "  account: chansung.tester.1007@gmail.com\n",
            "  project: central-hangar-321813\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [gde] with new settings \n",
            " [2] Create a new configuration\n",
            " [3] Switch to and re-initialize existing configuration: [default]\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [gde]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for \n",
            "this configuration:\n",
            " [1] chansung.tester.1007@gmail.com\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [chansung.tester.1007@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] central-hangar-321813\n",
            " [2] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  1\n",
            "\n",
            "Your current project has been set to: [central-hangar-321813].\n",
            "\n",
            "Do you want to configure a default Compute Region and Zone? (Y/n)?  Y\n",
            "\n",
            "Which Google Compute Engine zone would you like to use as project \n",
            "default?\n",
            "If you do not specify a zone via a command line flag while working \n",
            "with Compute Engine resources, the default is assumed.\n",
            " [1] us-east1-b\n",
            " [2] us-east1-c\n",
            " [3] us-east1-d\n",
            " [4] us-east4-c\n",
            " [5] us-east4-b\n",
            " [6] us-east4-a\n",
            " [7] us-central1-c\n",
            " [8] us-central1-a\n",
            " [9] us-central1-f\n",
            " [10] us-central1-b\n",
            " [11] us-west1-b\n",
            " [12] us-west1-c\n",
            " [13] us-west1-a\n",
            " [14] europe-west4-a\n",
            " [15] europe-west4-b\n",
            " [16] europe-west4-c\n",
            " [17] europe-west1-b\n",
            " [18] europe-west1-d\n",
            " [19] europe-west1-c\n",
            " [20] europe-west3-c\n",
            " [21] europe-west3-a\n",
            " [22] europe-west3-b\n",
            " [23] europe-west2-c\n",
            " [24] europe-west2-b\n",
            " [25] europe-west2-a\n",
            " [26] asia-east1-b\n",
            " [27] asia-east1-a\n",
            " [28] asia-east1-c\n",
            " [29] asia-southeast1-b\n",
            " [30] asia-southeast1-a\n",
            " [31] asia-southeast1-c\n",
            " [32] asia-northeast1-b\n",
            " [33] asia-northeast1-c\n",
            " [34] asia-northeast1-a\n",
            " [35] asia-south1-c\n",
            " [36] asia-south1-b\n",
            " [37] asia-south1-a\n",
            " [38] australia-southeast1-b\n",
            " [39] australia-southeast1-c\n",
            " [40] australia-southeast1-a\n",
            " [41] southamerica-east1-b\n",
            " [42] southamerica-east1-c\n",
            " [43] southamerica-east1-a\n",
            " [44] asia-east2-a\n",
            " [45] asia-east2-b\n",
            " [46] asia-east2-c\n",
            " [47] asia-northeast2-a\n",
            " [48] asia-northeast2-b\n",
            " [49] asia-northeast2-c\n",
            " [50] asia-northeast3-a\n",
            "Did not print [36] options.\n",
            "Too many options [86]. Enter \"list\" at prompt to print choices fully.\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  8\n",
            "\n",
            "Your project default Compute Engine zone has been set to [us-central1-a].\n",
            "You can change it by running [gcloud config set compute/zone NAME].\n",
            "\n",
            "Your project default Compute Engine region has been set to [us-central1].\n",
            "You can change it by running [gcloud config set compute/region NAME].\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use chansung.tester.1007@gmail.com by default\n",
            "* Commands will reference project `central-hangar-321813` by default\n",
            "* Compute Engine commands will use region `us-central1` by default\n",
            "* Compute Engine commands will use zone `us-central1-a` by default\n",
            "\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [gde]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl8ewjX3oXRx"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqVWpmywXngD"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wptXF0e-UXsT",
        "outputId": "ce784330-f56d-408e-bd7a-5549cda51093"
      },
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))\n",
        "import kfp\n",
        "print('KFP version: {}'.format(kfp.__version__))\n",
        "\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "import os"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.5.1\n",
            "TFX version: 1.2.0\n",
            "KFP version: 1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFYHeepnXxpZ"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPVyBrXrW-vu"
      },
      "source": [
        "GOOGLE_CLOUD_PROJECT = 'central-hangar-321813'    #@param {type:\"string\"}\n",
        "GOOGLE_CLOUD_REGION = 'us-central1'             #@param {type:\"string\"}\n",
        "GCS_BUCKET_NAME = 'cifar10-experimental-csp'            #@param {type:\"string\"}\n",
        "DATA_ROOT = 'gs://cifar10-csp-public' #@param {type:\"string\"}\n",
        "\n",
        "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
        "    from absl import logging\n",
        "    logging.error('Please set all required parameters.')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV-BZSvQq7YY"
      },
      "source": [
        "The location of the bucket must be a single region. Also, the bucket needs to be created in a region when [Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J65KHrt4X-Fu",
        "outputId": "e89cce87-75b9-4c48-9f3a-04c5811282b5"
      },
      "source": [
        "PIPELINE_NAME = 'continuous-adaptation-for-data-changes'\n",
        "\n",
        "# Path to various pipeline artifact.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# Paths for users' Python module.\n",
        "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIPELINE_ROOT: gs://cifar10-experimental-csp/pipeline_root/continuous-adaptation-for-data-changes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQVpzyftX0y0"
      },
      "source": [
        "## Create training modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFgnx3uGAfuj"
      },
      "source": [
        "_trainer_module_file = 'trainer.py'"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZqzotkfAf-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ef5435-f967-435b-fc23-8291bab16f55"
      },
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "from tensorflow import keras\n",
        "from tfx import v1 as tfx\n",
        "import tensorflow as tf\n",
        "\n",
        "_IMAGE_FEATURES = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string),\n",
        "    \"label\": tf.io.FixedLenFeature([], tf.int64),\n",
        "}\n",
        "\n",
        "_CONCRETE_INPUT = \"numpy_inputs\"\n",
        "_TRAIN_BATCH_SIZE = 64\n",
        "_EVAL_BATCH_SIZE = 64\n",
        "_INPUT_SHAPE = (32, 32, 3)\n",
        "_EPOCHS = 2\n",
        "\n",
        "def _parse_fn(example):\n",
        "    example = tf.io.parse_single_example(example, _IMAGE_FEATURES)\n",
        "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
        "    class_label = tf.cast(example[\"label\"], tf.int32)\n",
        "    return image, class_label\n",
        "\n",
        "def _input_fn(file_pattern: List[str], batch_size: int) -> tf.data.Dataset:\n",
        "  print(f\"Reading data from: {file_pattern}\")\n",
        "  tfrecord_filenames = tf.io.gfile.glob(file_pattern[0] + \".gz\")\n",
        "  print(tfrecord_filenames)\n",
        "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "  dataset = dataset.map(_parse_fn).batch(batch_size)\n",
        "  return dataset.repeat()\n",
        "\n",
        "def _make_keras_model() -> tf.keras.Model:\n",
        "  \"\"\"Creates a ResNet50-based model for classifying flowers data.\n",
        "\n",
        "  Returns:\n",
        "  A Keras Model.\n",
        "  \"\"\"\n",
        "  inputs = keras.Input(shape=_INPUT_SHAPE)\n",
        "  base_model = keras.applications.ResNet50(\n",
        "      include_top=False, input_shape=_INPUT_SHAPE, pooling=\"avg\"\n",
        "  )\n",
        "  base_model.trainable = False\n",
        "  x = tf.keras.applications.resnet.preprocess_input(inputs)\n",
        "  x = base_model(\n",
        "      x, training=False\n",
        "  )  # Ensures BatchNorm runs in inference model in this model\n",
        "  outputs = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "  )\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "def _preprocess(bytes_input):\n",
        "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
        "    resized = tf.image.resize(decoded, size=(32, 32))\n",
        "    return resized\n",
        "\n",
        "\n",
        "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
        "def preprocess_fn(bytes_inputs):\n",
        "    decoded_images = tf.map_fn(\n",
        "        _preprocess, bytes_inputs, dtype=tf.float32, back_prop=False\n",
        "    )\n",
        "    return {_CONCRETE_INPUT: decoded_images}\n",
        "\n",
        "\n",
        "def _model_exporter(model: tf.keras.Model):\n",
        "  m_call = tf.function(model.call).get_concrete_function(\n",
        "      [\n",
        "          tf.TensorSpec(\n",
        "              shape=[None, 32, 32, 3], dtype=tf.float32, name=_CONCRETE_INPUT\n",
        "          )\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
        "  def serving_fn(bytes_inputs):\n",
        "    # This function comes from the Computer Vision book from O'Reilly.\n",
        "    labels = tf.constant(\n",
        "        [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"], dtype=tf.string\n",
        "    )\n",
        "    images = preprocess_fn(bytes_inputs)\n",
        "\n",
        "    probs = m_call(**images)\n",
        "    indices = tf.argmax(probs, axis=1)\n",
        "    pred_source = tf.gather(params=labels, indices=indices)\n",
        "    pred_confidence = tf.reduce_max(probs, axis=1)\n",
        "    return {\"label\": pred_source, \"confidence\": pred_confidence}\n",
        "\n",
        "  return serving_fn\n",
        "\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  print(fn_args)\n",
        "\n",
        "  train_dataset = _input_fn(fn_args.train_files, batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(fn_args.eval_files, batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  model = _make_keras_model()\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps,\n",
        "      epochs=_EPOCHS,\n",
        "  )  \n",
        "\n",
        "  _, acc = model.evaluate(eval_dataset, steps=fn_args.eval_steps)\n",
        "  logging.info(f\"Validation accuracy: {round(acc * 100, 2)}%\")\n",
        "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
        "  # directory.\n",
        "  tf.saved_model.save(\n",
        "      model,\n",
        "      fn_args.serving_model_dir,\n",
        "      signatures={\"serving_default\": _model_exporter(model)},\n",
        "  )  "
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trainer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEPD_70MLf9b",
        "outputId": "9201cece-3766-404d-bc48-428e2ab6f96c"
      },
      "source": [
        "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/\n",
        "!gsutil ls -lh {MODULE_ROOT}/"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://trainer.py [Content-Type=text/x-python]...\n",
            "/ [1 files][  3.8 KiB/  3.8 KiB]                                                \n",
            "Operation completed over 1 objects/3.8 KiB.                                      \n",
            "   3.8 KiB  2021-09-16T16:47:25Z  gs://cifar10-experimental-csp/pipeline_module/continuous-adaptation-for-data-changes/trainer.py\n",
            "TOTAL: 1 objects, 3890 bytes (3.8 KiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uKK1LHdaNIJc",
        "outputId": "755781db-c054-438c-8813-07b3e23a119a"
      },
      "source": [
        "os.path.join(MODULE_ROOT, _trainer_module_file)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gs://cifar10-experimental-csp/pipeline_module/continuous-adaptation-for-data-changes/trainer.py'"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WTt_mw3cnia"
      },
      "source": [
        "## Custom Vertex Components \n",
        "- basically cloned from [Dual Deployment Project]()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7XUOglgctyb"
      },
      "source": [
        "_vertex_uploader_module_file = 'vertex_uploader.py'\n",
        "_vertex_deployer_module_file = 'vertex_deployer.py'"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnOtYLm6cviP",
        "outputId": "3ed83b75-a691-4091-84d0-fb269728762a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile {_vertex_uploader_module_file}\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.types.standard_artifacts import String\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from tfx import v1 as tfx\n",
        "from absl import logging\n",
        "\n",
        "\n",
        "@component\n",
        "def VertexUploader(\n",
        "    project: Parameter[str],\n",
        "    region: Parameter[str],\n",
        "    model_display_name: Parameter[str],\n",
        "    pushed_model_location: Parameter[str],\n",
        "    serving_image_uri: Parameter[str],\n",
        "    uploaded_model: tfx.dsl.components.OutputArtifact[String],\n",
        "):\n",
        "\n",
        "    vertex_ai.init(project=project, location=region)\n",
        "\n",
        "    pushed_model_dir = os.path.join(\n",
        "        pushed_model_location, tf.io.gfile.listdir(pushed_model_location)[-1]\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Model registry location: {pushed_model_dir}\")\n",
        "\n",
        "    vertex_model = vertex_ai.Model.upload(\n",
        "        display_name=model_display_name,\n",
        "        artifact_uri=pushed_model_dir,\n",
        "        serving_container_image_uri=serving_image_uri,\n",
        "        parameters_schema_uri=None,\n",
        "        instance_schema_uri=None,\n",
        "        explanation_metadata=None,\n",
        "        explanation_parameters=None,\n",
        "    )\n",
        "\n",
        "    uploaded_model.set_string_custom_property(\n",
        "        \"model_resource_name\", str(vertex_model.resource_name)\n",
        "    )\n",
        "    logging.info(f\"Model resource: {str(vertex_model.resource_name)}\")\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vertex_uploader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nUaRKSJczio",
        "outputId": "22f61dac-666c-4844-ea6c-e722af1eae3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile {_vertex_deployer_module_file}\n",
        "\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.types.standard_artifacts import String\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from tfx import v1 as tfx\n",
        "from absl import logging\n",
        "\n",
        "\n",
        "@component\n",
        "def VertexDeployer(\n",
        "    project: Parameter[str],\n",
        "    region: Parameter[str],\n",
        "    model_display_name: Parameter[str],\n",
        "    deployed_model_display_name: Parameter[str],\n",
        "):\n",
        "\n",
        "    logging.info(f\"Endpoint display: {deployed_model_display_name}\")\n",
        "    vertex_ai.init(project=project, location=region)\n",
        "\n",
        "    endpoints = vertex_ai.Endpoint.list(\n",
        "        filter=f\"display_name={deployed_model_display_name}\", order_by=\"update_time\"\n",
        "    )\n",
        "\n",
        "    if len(endpoints) > 0:\n",
        "        logging.info(f\"Endpoint {deployed_model_display_name} already exists.\")\n",
        "        endpoint = endpoints[-1]\n",
        "    else:\n",
        "        endpoint = vertex_ai.Endpoint.create(deployed_model_display_name)\n",
        "\n",
        "    model = vertex_ai.Model.list(\n",
        "        filter=f\"display_name={model_display_name}\", order_by=\"update_time\"\n",
        "    )[-1]\n",
        "\n",
        "    endpoint = vertex_ai.Endpoint.list(\n",
        "        filter=f\"display_name={deployed_model_display_name}\", order_by=\"update_time\"\n",
        "    )[-1]\n",
        "\n",
        "    deployed_model = endpoint.deploy(\n",
        "        model=model,\n",
        "        # Syntax from here: https://git.io/JBQDP\n",
        "        traffic_split={\"0\": 100},\n",
        "        machine_type=\"n1-standard-4\",\n",
        "        min_replica_count=1,\n",
        "        max_replica_count=1,\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Model deployed to: {deployed_model}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vertex_deployer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyR80VnedA9Y"
      },
      "source": [
        "!mkdir -p ./custom_components\n",
        "!touch ./custom_components/__init__.py\n",
        "!cp -r {_vertex_uploader_module_file} {_vertex_deployer_module_file} custom_components"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLXV-aRodEmH",
        "outputId": "96647582-cc8e-4fbf-a24f-4b3f5f300a8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls -lh custom_components"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8.0K\n",
            "-rw-r--r-- 1 root root    0 Sep 16 16:33 __init__.py\n",
            "-rw-r--r-- 1 root root 1.5K Sep 16 16:33 vertex_deployer.py\n",
            "-rw-r--r-- 1 root root 1.4K Sep 16 16:33 vertex_uploader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-un8Vj1dGoL",
        "outputId": "97ef1660-4dc1-4f26-c2c5-03e3306f4e3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "DATASET_DISPLAY_NAME = \"cifar10\"\n",
        "VERSION = \"tfx-1-2-0\"\n",
        "TFX_IMAGE_URI = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
        "print(f\"URI of the custom image: {TFX_IMAGE_URI}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URI of the custom image: gcr.io/central-hangar-321813/cifar10:tfx-1-2-0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95lKF_6QdQ4o",
        "outputId": "3ed4d45d-c879-414d-c650-f629617ac813",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
        "RUN mkdir -p custom_components\n",
        "COPY custom_components/* ./custom_components/\n",
        "RUN pip install --upgrade google-cloud-aiplatform"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc_K3jVCdXE8",
        "outputId": "68058a12-1314-40b7-acdb-9d0236d93062",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary tarball archive of 38 file(s) totalling 54.3 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://central-hangar-321813_cloudbuild/source/1631810333.801335-3c41a723a93741fdb7f485dae0eb3d64.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/central-hangar-321813/locations/global/builds/1960abbf-4d56-4ad9-861a-74487c2563e0].\n",
            "Logs are available at [https://console.cloud.google.com/cloud-build/builds/1960abbf-4d56-4ad9-861a-74487c2563e0?project=31482268105].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"1960abbf-4d56-4ad9-861a-74487c2563e0\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://central-hangar-321813_cloudbuild/source/1631810333.801335-3c41a723a93741fdb7f485dae0eb3d64.tgz#1631810343944490\n",
            "Copying gs://central-hangar-321813_cloudbuild/source/1631810333.801335-3c41a723a93741fdb7f485dae0eb3d64.tgz#1631810343944490...\n",
            "/ [1 files][  6.5 MiB/  6.5 MiB]                                                \n",
            "Operation completed over 1 objects/6.5 MiB.\n",
            "tar: .config/gce: time stamp 2040-01-01 00:00:00 is 577178383.775819738 s in the future\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  56.99MB\n",
            "Step 1/4 : FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
            "1.2.0: Pulling from tfx-oss-public/tfx\n",
            "25fa05cd42bd: Pulling fs layer\n",
            "2d6e353a95ec: Pulling fs layer\n",
            "14d7996407de: Pulling fs layer\n",
            "0c9c6fc70f16: Pulling fs layer\n",
            "c3c76be11512: Pulling fs layer\n",
            "ab6e5a9c78ee: Pulling fs layer\n",
            "7bc1690abd59: Pulling fs layer\n",
            "f5b4dd7682bc: Pulling fs layer\n",
            "d6897660f71d: Pulling fs layer\n",
            "174d792fb622: Pulling fs layer\n",
            "5f8143275aca: Pulling fs layer\n",
            "56646f115483: Pulling fs layer\n",
            "798922b52524: Pulling fs layer\n",
            "e2699a9f592b: Pulling fs layer\n",
            "f43e7d1c07e4: Pulling fs layer\n",
            "1e71d5e9923d: Pulling fs layer\n",
            "bf6ae2a2e250: Pulling fs layer\n",
            "e49679b748d5: Pulling fs layer\n",
            "80208bd6f7fb: Pulling fs layer\n",
            "b83c16bef138: Pulling fs layer\n",
            "9d1427033824: Pulling fs layer\n",
            "c0028679f003: Pulling fs layer\n",
            "09c222e7ff04: Pulling fs layer\n",
            "ae6048a3aec1: Pulling fs layer\n",
            "0c9c6fc70f16: Waiting\n",
            "1ced637de50b: Pulling fs layer\n",
            "762ff1eb7f16: Pulling fs layer\n",
            "c3c76be11512: Waiting\n",
            "f6f8f4265c8c: Pulling fs layer\n",
            "595b1c49222a: Pulling fs layer\n",
            "b6c7eb38f366: Pulling fs layer\n",
            "520be5017b4d: Pulling fs layer\n",
            "ab6e5a9c78ee: Waiting\n",
            "0a1aacb7e387: Pulling fs layer\n",
            "7bc1690abd59: Waiting\n",
            "8f605134caf9: Pulling fs layer\n",
            "189ba322d030: Pulling fs layer\n",
            "f5b4dd7682bc: Waiting\n",
            "af92447b9198: Pulling fs layer\n",
            "d6897660f71d: Waiting\n",
            "56646f115483: Waiting\n",
            "174d792fb622: Waiting\n",
            "798922b52524: Waiting\n",
            "5f8143275aca: Waiting\n",
            "e2699a9f592b: Waiting\n",
            "f43e7d1c07e4: Waiting\n",
            "1e71d5e9923d: Waiting\n",
            "f6f8f4265c8c: Waiting\n",
            "8f605134caf9: Waiting\n",
            "189ba322d030: Waiting\n",
            "595b1c49222a: Waiting\n",
            "af92447b9198: Waiting\n",
            "b6c7eb38f366: Waiting\n",
            "520be5017b4d: Waiting\n",
            "0a1aacb7e387: Waiting\n",
            "bf6ae2a2e250: Waiting\n",
            "c0028679f003: Waiting\n",
            "e49679b748d5: Waiting\n",
            "09c222e7ff04: Waiting\n",
            "80208bd6f7fb: Waiting\n",
            "9d1427033824: Waiting\n",
            "ae6048a3aec1: Waiting\n",
            "b83c16bef138: Waiting\n",
            "1ced637de50b: Waiting\n",
            "762ff1eb7f16: Waiting\n",
            "2d6e353a95ec: Download complete\n",
            "0c9c6fc70f16: Download complete\n",
            "25fa05cd42bd: Verifying Checksum\n",
            "25fa05cd42bd: Download complete\n",
            "c3c76be11512: Download complete\n",
            "7bc1690abd59: Verifying Checksum\n",
            "7bc1690abd59: Download complete\n",
            "14d7996407de: Verifying Checksum\n",
            "14d7996407de: Download complete\n",
            "d6897660f71d: Verifying Checksum\n",
            "d6897660f71d: Download complete\n",
            "25fa05cd42bd: Pull complete\n",
            "2d6e353a95ec: Pull complete\n",
            "14d7996407de: Pull complete\n",
            "0c9c6fc70f16: Pull complete\n",
            "c3c76be11512: Pull complete\n",
            "f5b4dd7682bc: Download complete\n",
            "5f8143275aca: Verifying Checksum\n",
            "5f8143275aca: Download complete\n",
            "ab6e5a9c78ee: Verifying Checksum\n",
            "ab6e5a9c78ee: Download complete\n",
            "798922b52524: Verifying Checksum\n",
            "798922b52524: Download complete\n",
            "e2699a9f592b: Verifying Checksum\n",
            "e2699a9f592b: Download complete\n",
            "f43e7d1c07e4: Verifying Checksum\n",
            "f43e7d1c07e4: Download complete\n",
            "56646f115483: Verifying Checksum\n",
            "56646f115483: Download complete\n",
            "bf6ae2a2e250: Verifying Checksum\n",
            "bf6ae2a2e250: Download complete\n",
            "e49679b748d5: Verifying Checksum\n",
            "e49679b748d5: Download complete\n",
            "80208bd6f7fb: Verifying Checksum\n",
            "80208bd6f7fb: Download complete\n",
            "b83c16bef138: Verifying Checksum\n",
            "b83c16bef138: Download complete\n",
            "9d1427033824: Verifying Checksum\n",
            "9d1427033824: Download complete\n",
            "c0028679f003: Verifying Checksum\n",
            "c0028679f003: Download complete\n",
            "09c222e7ff04: Verifying Checksum\n",
            "09c222e7ff04: Download complete\n",
            "ae6048a3aec1: Verifying Checksum\n",
            "ae6048a3aec1: Download complete\n",
            "1e71d5e9923d: Download complete\n",
            "1ced637de50b: Download complete\n",
            "f6f8f4265c8c: Verifying Checksum\n",
            "f6f8f4265c8c: Download complete\n",
            "174d792fb622: Verifying Checksum\n",
            "174d792fb622: Download complete\n",
            "b6c7eb38f366: Verifying Checksum\n",
            "b6c7eb38f366: Download complete\n",
            "520be5017b4d: Download complete\n",
            "0a1aacb7e387: Verifying Checksum\n",
            "0a1aacb7e387: Download complete\n",
            "8f605134caf9: Verifying Checksum\n",
            "8f605134caf9: Download complete\n",
            "189ba322d030: Verifying Checksum\n",
            "189ba322d030: Download complete\n",
            "595b1c49222a: Verifying Checksum\n",
            "595b1c49222a: Download complete\n",
            "af92447b9198: Verifying Checksum\n",
            "af92447b9198: Download complete\n",
            "762ff1eb7f16: Verifying Checksum\n",
            "762ff1eb7f16: Download complete\n",
            "ab6e5a9c78ee: Pull complete\n",
            "7bc1690abd59: Pull complete\n",
            "f5b4dd7682bc: Pull complete\n",
            "d6897660f71d: Pull complete\n",
            "174d792fb622: Pull complete\n",
            "5f8143275aca: Pull complete\n",
            "56646f115483: Pull complete\n",
            "798922b52524: Pull complete\n",
            "e2699a9f592b: Pull complete\n",
            "f43e7d1c07e4: Pull complete\n",
            "1e71d5e9923d: Pull complete\n",
            "bf6ae2a2e250: Pull complete\n",
            "e49679b748d5: Pull complete\n",
            "80208bd6f7fb: Pull complete\n",
            "b83c16bef138: Pull complete\n",
            "9d1427033824: Pull complete\n",
            "c0028679f003: Pull complete\n",
            "09c222e7ff04: Pull complete\n",
            "ae6048a3aec1: Pull complete\n",
            "1ced637de50b: Pull complete\n",
            "762ff1eb7f16: Pull complete\n",
            "f6f8f4265c8c: Pull complete\n",
            "595b1c49222a: Pull complete\n",
            "b6c7eb38f366: Pull complete\n",
            "520be5017b4d: Pull complete\n",
            "0a1aacb7e387: Pull complete\n",
            "8f605134caf9: Pull complete\n",
            "189ba322d030: Pull complete\n",
            "af92447b9198: Pull complete\n",
            "Digest: sha256:eba9e7b7d9131eb5b05434feaafc4676268ad805e4b97218f58994ad2714be67\n",
            "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.2.0\n",
            " ---> 0e86fadcc60c\n",
            "Step 2/4 : RUN mkdir -p custom_components\n",
            " ---> Running in 5476d19db059\n",
            "Removing intermediate container 5476d19db059\n",
            " ---> c0a7b9062ea7\n",
            "Step 3/4 : COPY custom_components/* ./custom_components/\n",
            " ---> b9993a0cd62b\n",
            "Step 4/4 : RUN pip install --upgrade google-cloud-aiplatform\n",
            " ---> Running in 510801942fac\n",
            "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (0.7.1)\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.4.2-py2.py3-none-any.whl (1.4 MB)\n",
            "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.19.0)\n",
            "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.31.1)\n",
            "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.41.1)\n",
            "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (20.9)\n",
            "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.20.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.25.1)\n",
            "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (49.6.0.post20210108)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.34.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.16.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.34.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.2.7)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.7.2)\n",
            "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.2)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.26.6)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.5.30)\n",
            "Installing collected packages: google-cloud-aiplatform\n",
            "  Attempting uninstall: google-cloud-aiplatform\n",
            "    Found existing installation: google-cloud-aiplatform 0.7.1\n",
            "    Uninstalling google-cloud-aiplatform-0.7.1:\n",
            "      Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
            "Successfully installed google-cloud-aiplatform-1.4.2\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tfx 1.2.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.4.2 which is incompatible.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "Removing intermediate container 510801942fac\n",
            " ---> 2ffc46a8ebe3\n",
            "Successfully built 2ffc46a8ebe3\n",
            "Successfully tagged gcr.io/central-hangar-321813/cifar10:tfx-1-2-0\n",
            "PUSH\n",
            "Pushing gcr.io/central-hangar-321813/cifar10:tfx-1-2-0\n",
            "The push refers to repository [gcr.io/central-hangar-321813/cifar10]\n",
            "164a897f9267: Preparing\n",
            "270cd492cc0e: Preparing\n",
            "1e0bf7b7ebf7: Preparing\n",
            "d42c05f73feb: Preparing\n",
            "3119d30f29a9: Preparing\n",
            "121c9dfc7bce: Preparing\n",
            "868786b3710b: Preparing\n",
            "5bb1aa5df10d: Preparing\n",
            "f028010939aa: Preparing\n",
            "dc99c4ea3a81: Preparing\n",
            "37b508c5711b: Preparing\n",
            "756ab564e194: Preparing\n",
            "2ae86808a3d1: Preparing\n",
            "1dccbdf9b557: Preparing\n",
            "cfcbdbc2b748: Preparing\n",
            "937ab8f29c2e: Preparing\n",
            "5d417b2f7486: Preparing\n",
            "d6a297a3e6e4: Preparing\n",
            "6474a5e8117f: Preparing\n",
            "fe498124ed57: Preparing\n",
            "d5454704bb3d: Preparing\n",
            "fb896ef24b4b: Preparing\n",
            "5087113f67c8: Preparing\n",
            "2a92857a1d48: Preparing\n",
            "0ded97864c52: Preparing\n",
            "b50bbaac3e32: Preparing\n",
            "262ea1af4c10: Preparing\n",
            "b420a468ca49: Preparing\n",
            "608c205798d1: Preparing\n",
            "0760cd6d4269: Preparing\n",
            "fb4755c89c2a: Preparing\n",
            "22cfb9034da6: Preparing\n",
            "8bec4fbfce85: Preparing\n",
            "3b129ca3db46: Preparing\n",
            "64cb1a1930ab: Preparing\n",
            "600ef5a43f1f: Preparing\n",
            "8f8f0266f834: Preparing\n",
            "262ea1af4c10: Waiting\n",
            "937ab8f29c2e: Waiting\n",
            "b420a468ca49: Waiting\n",
            "5d417b2f7486: Waiting\n",
            "608c205798d1: Waiting\n",
            "d6a297a3e6e4: Waiting\n",
            "0760cd6d4269: Waiting\n",
            "fb4755c89c2a: Waiting\n",
            "6474a5e8117f: Waiting\n",
            "22cfb9034da6: Waiting\n",
            "600ef5a43f1f: Waiting\n",
            "2a92857a1d48: Waiting\n",
            "8bec4fbfce85: Waiting\n",
            "8f8f0266f834: Waiting\n",
            "0ded97864c52: Waiting\n",
            "3b129ca3db46: Waiting\n",
            "64cb1a1930ab: Waiting\n",
            "b50bbaac3e32: Waiting\n",
            "fe498124ed57: Waiting\n",
            "fb896ef24b4b: Waiting\n",
            "d5454704bb3d: Waiting\n",
            "5087113f67c8: Waiting\n",
            "121c9dfc7bce: Waiting\n",
            "868786b3710b: Waiting\n",
            "f028010939aa: Waiting\n",
            "756ab564e194: Waiting\n",
            "2ae86808a3d1: Waiting\n",
            "dc99c4ea3a81: Waiting\n",
            "1dccbdf9b557: Waiting\n",
            "37b508c5711b: Waiting\n",
            "cfcbdbc2b748: Waiting\n",
            "5bb1aa5df10d: Waiting\n",
            "3119d30f29a9: Layer already exists\n",
            "d42c05f73feb: Layer already exists\n",
            "121c9dfc7bce: Layer already exists\n",
            "868786b3710b: Layer already exists\n",
            "5bb1aa5df10d: Layer already exists\n",
            "f028010939aa: Layer already exists\n",
            "dc99c4ea3a81: Layer already exists\n",
            "37b508c5711b: Layer already exists\n",
            "756ab564e194: Layer already exists\n",
            "2ae86808a3d1: Layer already exists\n",
            "1dccbdf9b557: Layer already exists\n",
            "cfcbdbc2b748: Layer already exists\n",
            "937ab8f29c2e: Layer already exists\n",
            "5d417b2f7486: Layer already exists\n",
            "d6a297a3e6e4: Layer already exists\n",
            "6474a5e8117f: Layer already exists\n",
            "fe498124ed57: Layer already exists\n",
            "d5454704bb3d: Layer already exists\n",
            "fb896ef24b4b: Layer already exists\n",
            "5087113f67c8: Layer already exists\n",
            "2a92857a1d48: Layer already exists\n",
            "0ded97864c52: Layer already exists\n",
            "b50bbaac3e32: Layer already exists\n",
            "262ea1af4c10: Layer already exists\n",
            "b420a468ca49: Layer already exists\n",
            "608c205798d1: Layer already exists\n",
            "0760cd6d4269: Layer already exists\n",
            "fb4755c89c2a: Layer already exists\n",
            "22cfb9034da6: Layer already exists\n",
            "8bec4fbfce85: Layer already exists\n",
            "3b129ca3db46: Layer already exists\n",
            "64cb1a1930ab: Layer already exists\n",
            "600ef5a43f1f: Layer already exists\n",
            "8f8f0266f834: Layer already exists\n",
            "1e0bf7b7ebf7: Pushed\n",
            "270cd492cc0e: Pushed\n",
            "164a897f9267: Pushed\n",
            "tfx-1-2-0: digest: sha256:033d17539e78f157a64a916e0de20d7ce43b5ebd2c299f46004282ac20c7ced1 size: 8100\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                          STATUS\n",
            "1960abbf-4d56-4ad9-861a-74487c2563e0  2021-09-16T16:39:04+00:00  4M11S     gs://central-hangar-321813_cloudbuild/source/1631810333.801335-3c41a723a93741fdb7f485dae0eb3d64.tgz  gcr.io/central-hangar-321813/cifar10:tfx-1-2-0  SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twstEC5PdjRq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGJU5sXrrAJW"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEbNM9CeERX2"
      },
      "source": [
        "# Specify training worker configurations. To minimize costs we can even specify two\n",
        "# different configurations: a beefier machine for the Endpoint model and slightly less\n",
        "# powerful machine for the mobile model.\n",
        "TRAINING_JOB_SPEC = {\n",
        "    'project': GOOGLE_CLOUD_PROJECT,\n",
        "    'worker_pool_specs': [{\n",
        "        'machine_spec': {\n",
        "            'machine_type': 'n1-standard-4',\n",
        "            'accelerator_type': 'NVIDIA_TESLA_K80',\n",
        "            'accelerator_count': 1\n",
        "        },\n",
        "        'replica_count': 1,\n",
        "        'container_spec': {\n",
        "            'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
        "        },\n",
        "    }],\n",
        "}"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivc6LzpVuzKb"
      },
      "source": [
        "SERVING_JOB_SPEC = {\n",
        "    'endpoint_name': PIPELINE_NAME.replace('-','_'),  # '-' is not allowed.\n",
        "    'project_id': GOOGLE_CLOUD_PROJECT,\n",
        "    'min_replica_count': 1,\n",
        "    'max_replica_count': 1,    \n",
        "    'machine_type': 'n1-standard-2',\n",
        "}"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOteqi0td5Vu"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln1cvbcfphA9"
      },
      "source": [
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.components.example_gen import utils\n",
        "\n",
        "from custom_components.vertex_uploader import VertexUploader\n",
        "from custom_components.vertex_deployer import VertexDeployer\n",
        "\n",
        "def _create_pipeline(\n",
        "    pipeline_name: str,\n",
        "    pipeline_root: str,\n",
        "    data_root: str,\n",
        "    serving_model_dir: str,\n",
        "    trainer_module: str,\n",
        "    project_id: str,\n",
        "    region: str,\n",
        ") -> tfx.dsl.Pipeline:\n",
        "    \"\"\"Creates a three component flowers pipeline with TFX.\"\"\"\n",
        "    splits = [\n",
        "      example_gen_pb2.Input.Split(name='train',pattern='span-{SPAN}/train/*'),\n",
        "      example_gen_pb2.Input.Split(name='val',pattern='span-{SPAN}/test/*')\n",
        "    ]\n",
        "    _, span, version = utils.calculate_splits_fingerprint_span_and_version(data_root, splits)\n",
        "\n",
        "    input_config = example_gen_pb2.Input(splits=[\n",
        "      example_gen_pb2.Input.Split(name='train', pattern=f'span-{span}/train/*'),\n",
        "                  example_gen_pb2.Input.Split(name='val', pattern=f'span-{span}/test/*')\n",
        "    ])\n",
        "    example_gen = tfx.components.ImportExampleGen(input_base=data_root,\n",
        "                                                  input_config=input_config)\n",
        "\n",
        "    # Trainer\n",
        "    trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
        "        module_file=trainer_module,\n",
        "        examples=example_gen.outputs[\"examples\"],\n",
        "        train_args=tfx.proto.TrainArgs(splits=['train'], num_steps=50000//64),\n",
        "        eval_args=tfx.proto.EvalArgs(splits=['val'], num_steps=10000//64),\n",
        "        custom_config={\n",
        "            tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY: True,\n",
        "            tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY: region,\n",
        "            tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY: TRAINING_JOB_SPEC,\n",
        "            \"use_gpu\": True,\n",
        "        },        \n",
        "    ).with_id(\"trainer\")\n",
        "\n",
        "    # Pushes the model to a filesystem destination.\n",
        "    pushed_model_location = os.path.join(serving_model_dir, \"resnet50\")\n",
        "    resnet_pusher = tfx.components.Pusher(\n",
        "        model=trainer.outputs[\"model\"],\n",
        "        push_destination=tfx.proto.PushDestination(\n",
        "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "                base_directory=pushed_model_location\n",
        "            )\n",
        "        ),\n",
        "    ).with_id(\"resnet_pusher\")\n",
        "\n",
        "    # Vertex AI upload.\n",
        "    model_display_name = \"resnet_cifar_latest\"\n",
        "    uploader = VertexUploader(\n",
        "        project=project_id,\n",
        "        region=region,\n",
        "        model_display_name=model_display_name,\n",
        "        pushed_model_location=pushed_model_location,\n",
        "        serving_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest\",\n",
        "    ).with_id(\"vertex_uploader\")\n",
        "    uploader.add_upstream_node(resnet_pusher)\n",
        "\n",
        "    # Create an endpoint.\n",
        "    deployer = VertexDeployer(\n",
        "        project=project_id,\n",
        "        region=region,\n",
        "        model_display_name=model_display_name,\n",
        "        deployed_model_display_name=model_display_name + \"_\" + TIMESTAMP,\n",
        "    ).with_id(\"vertex_deployer\")\n",
        "    deployer.add_upstream_node(uploader)\n",
        "\n",
        "    # pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
        "    #     model=trainer.outputs['model'],\n",
        "    #     custom_config={\n",
        "    #         tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY: True,\n",
        "    #         tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY: region,\n",
        "    #         tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY: 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest',\n",
        "    #         tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY: SERVING_JOB_SPEC\n",
        "    #     }\n",
        "    # ).with_id('pusher')\n",
        "\n",
        "    components = [\n",
        "        example_gen,\n",
        "        trainer,\n",
        "        resnet_pusher,\n",
        "        uploader,\n",
        "        deployer,\n",
        "    ]\n",
        "\n",
        "    return tfx.dsl.Pipeline(\n",
        "        pipeline_name=pipeline_name, \n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components\n",
        "    )\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFdlslfOX54z"
      },
      "source": [
        "## Compile the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AY5Z2tbsbwE"
      },
      "source": [
        "import os\n",
        "\n",
        "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
        "\n",
        "# Important: We need to pass the custom Docker image URI to the\n",
        "# `KubeflowV2DagRunnerConfig` to take effect.\n",
        "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
        "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(default_image=TFX_IMAGE_URI),\n",
        "    output_filename=PIPELINE_DEFINITION_FILE)\n",
        "\n",
        "_ = runner.run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        data_root=DATA_ROOT,\n",
        "        serving_model_dir=SERVING_MODEL_DIR,\n",
        "        trainer_module=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
        "        project_id=GOOGLE_CLOUD_PROJECT,\n",
        "        region=GOOGLE_CLOUD_REGION\n",
        "    )\n",
        ")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHBJaR_X7x2"
      },
      "source": [
        "## Submit the pipeline for execution to Vertex AI\n",
        "\n",
        "Generally, it's a good idea to first do a local run of the end-to-end pipeline before submitting it an online orchestrator. We can use `tfx.orchestration.LocalDagRunner()` for that but for the purposes of this notebook we won't be doing that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "3elrtDOus83z",
        "outputId": "38fa465e-397b-4d9a-9ef8-342ebf41be6c"
      },
      "source": [
        "from kfp.v2.google import client\n",
        "\n",
        "pipelines_client = client.AIPlatformClient(\n",
        "    project_id=GOOGLE_CLOUD_PROJECT,\n",
        "    region=GOOGLE_CLOUD_REGION,\n",
        ")\n",
        "\n",
        "_ = pipelines_client.create_run_from_job_spec(PIPELINE_DEFINITION_FILE, enable_caching=True)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/continuous-adaptation-for-data-changes-20210916164809?project=central-hangar-321813\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvZWYHfRCwFz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}