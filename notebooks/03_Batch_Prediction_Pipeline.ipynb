{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "03_Batch_Prediction_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/03_Batch_Prediction_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqBUdDXK2IF"
      },
      "source": [
        "## Batch Prediction Pipeline\n",
        "\n",
        "The main purpose of this notebook is to build KFP pipeline doing the following steps\n",
        "1. Create an batch request input file (file list format) based on the files uploaded to a GCS bucket\n",
        "2. Run Batch Prediction on the trained model obtained from [02_TFX_Training_Pipeline.ipynb](https://github.com/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/02_TFX_Training_Pipeline.ipynb)\n",
        "3. Measure the batch prediction model performance in terms of accuracy\n",
        "4. If model performance < threshold\n",
        "  - Copy the testing images to the original(previous) dataset\n",
        "  - Trigger the TFX training pipeline with original data + newly added data\n",
        "\n",
        "The functional test for batch prediction is shown in a separate notebook, [98_Batch_Prediction_Test.ipynb](https://github.com/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/98_Batch_Prediction_Test.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuQerdmAK38q"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNC8roxxK3j5"
      },
      "source": [
        "!pip install fastdot\n",
        "!pip install tfx==1.2.0\n",
        "!pip install kfp==1.6.1\n",
        "!pip install -q --upgrade google-cloud-aiplatform\n",
        "!pip install -q --upgrade google-cloud-storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9309avHssn9O"
      },
      "source": [
        "!pip install -q ml-metadata"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuCgpQiwchZZ"
      },
      "source": [
        "### ***Restart runtime.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UKtYc2IK2II"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd9CvkmRLFdu"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omrlUvFbLXsv"
      },
      "source": [
        "## Custom TFX Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zMGdk-lM7QX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "cellView": "form",
        "outputId": "45dc748f-a612-4797-c9cc-ab6e574f002c"
      },
      "source": [
        "#@title\n",
        "from fastdot.core import *\n",
        "\n",
        "tfx_components = ['FileListGen', 'BatchPredictionGen', 'ModelPerformanceGen', 'PipelineTrigger']\n",
        "block = 'TFX Component Workflow'\n",
        "\n",
        "g = graph_items(seq_cluster(tfx_components, block))\n",
        "g"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pydot.Dot at 0x7fcda67db8d0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"619pt\" height=\"99pt\"\n viewBox=\"0.00 0.00 619.00 99.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 95)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-95 615,-95 615,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_nbb3b9233fce248648608c147c7c39169</title>\n<g id=\"a_clust1\"><a xlink:title=\"TFX Component Workflow\">\n<path fill=\"#555555\" fill-opacity=\"0.133333\" stroke=\"#000000\" d=\"M20,-8C20,-8 591,-8 591,-8 597,-8 603,-14 603,-20 603,-20 603,-71 603,-71 603,-77 597,-83 591,-83 591,-83 20,-83 20,-83 14,-83 8,-77 8,-71 8,-71 8,-20 8,-20 8,-14 14,-8 20,-8\"/>\n<text text-anchor=\"middle\" x=\"305.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TFX Component Workflow</text>\n</a>\n</g>\n</g>\n<!-- n7dc35327c9514d7197d65b678d45bb9e -->\n<g id=\"node1\" class=\"node\">\n<title>n7dc35327c9514d7197d65b678d45bb9e</title>\n<g id=\"a_node1\"><a xlink:title=\"FileListGen\">\n<path fill=\"#22d6e0\" fill-opacity=\"0.407843\" stroke=\"#000000\" d=\"M88,-52C88,-52 28,-52 28,-52 22,-52 16,-46 16,-40 16,-40 16,-28 16,-28 16,-22 22,-16 28,-16 28,-16 88,-16 88,-16 94,-16 100,-22 100,-28 100,-28 100,-40 100,-40 100,-46 94,-52 88,-52\"/>\n<text text-anchor=\"middle\" x=\"58\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FileListGen</text>\n</a>\n</g>\n</g>\n<!-- n4a1b1322b8de449fb68b6ab75bda1bfb -->\n<g id=\"node2\" class=\"node\">\n<title>n4a1b1322b8de449fb68b6ab75bda1bfb</title>\n<g id=\"a_node2\"><a xlink:title=\"BatchPredictionGen\">\n<path fill=\"#c6e789\" fill-opacity=\"0.462745\" stroke=\"#000000\" d=\"M256,-52C256,-52 148,-52 148,-52 142,-52 136,-46 136,-40 136,-40 136,-28 136,-28 136,-22 142,-16 148,-16 148,-16 256,-16 256,-16 262,-16 268,-22 268,-28 268,-28 268,-40 268,-40 268,-46 262,-52 256,-52\"/>\n<text text-anchor=\"middle\" x=\"202\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">BatchPredictionGen</text>\n</a>\n</g>\n</g>\n<!-- n7dc35327c9514d7197d65b678d45bb9e&#45;&gt;n4a1b1322b8de449fb68b6ab75bda1bfb -->\n<g id=\"edge1\" class=\"edge\">\n<title>n7dc35327c9514d7197d65b678d45bb9e&#45;&gt;n4a1b1322b8de449fb68b6ab75bda1bfb</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M100.0379,-34C108.0929,-34 116.7569,-34 125.5097,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.7944,-37.5001 135.7944,-34 125.7943,-30.5001 125.7944,-37.5001\"/>\n</g>\n<!-- n24a58e254cba4c858aa12c632590d444 -->\n<g id=\"node3\" class=\"node\">\n<title>n24a58e254cba4c858aa12c632590d444</title>\n<g id=\"a_node3\"><a xlink:title=\"ModelPerformanceGen\">\n<path fill=\"#b6f193\" fill-opacity=\"0.368627\" stroke=\"#000000\" d=\"M442,-52C442,-52 316,-52 316,-52 310,-52 304,-46 304,-40 304,-40 304,-28 304,-28 304,-22 310,-16 316,-16 316,-16 442,-16 442,-16 448,-16 454,-22 454,-28 454,-28 454,-40 454,-40 454,-46 448,-52 442,-52\"/>\n<text text-anchor=\"middle\" x=\"379\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ModelPerformanceGen</text>\n</a>\n</g>\n</g>\n<!-- n4a1b1322b8de449fb68b6ab75bda1bfb&#45;&gt;n24a58e254cba4c858aa12c632590d444 -->\n<g id=\"edge2\" class=\"edge\">\n<title>n4a1b1322b8de449fb68b6ab75bda1bfb&#45;&gt;n24a58e254cba4c858aa12c632590d444</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M268.16,-34C276.5585,-34 285.2441,-34 293.8867,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"293.9927,-37.5001 303.9927,-34 293.9927,-30.5001 293.9927,-37.5001\"/>\n</g>\n<!-- nd1cc86fad8d64f94b3d27f83bc62404b -->\n<g id=\"node4\" class=\"node\">\n<title>nd1cc86fad8d64f94b3d27f83bc62404b</title>\n<g id=\"a_node4\"><a xlink:title=\"PipelineTrigger\">\n<path fill=\"#2e7bf7\" fill-opacity=\"0.690196\" stroke=\"#000000\" d=\"M583,-52C583,-52 502,-52 502,-52 496,-52 490,-46 490,-40 490,-40 490,-28 490,-28 490,-22 496,-16 502,-16 502,-16 583,-16 583,-16 589,-16 595,-22 595,-28 595,-28 595,-40 595,-40 595,-46 589,-52 583,-52\"/>\n<text text-anchor=\"middle\" x=\"542.5\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PipelineTrigger</text>\n</a>\n</g>\n</g>\n<!-- n24a58e254cba4c858aa12c632590d444&#45;&gt;nd1cc86fad8d64f94b3d27f83bc62404b -->\n<g id=\"edge3\" class=\"edge\">\n<title>n24a58e254cba4c858aa12c632590d444&#45;&gt;nd1cc86fad8d64f94b3d27f83bc62404b</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M454.0506,-34C462.6448,-34 471.3384,-34 479.7481,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"479.8394,-37.5001 489.8394,-34 479.8394,-30.5001 479.8394,-37.5001\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN4wUPP4Laqk"
      },
      "source": [
        "### **FileListGen Component**\n",
        "\n",
        "- `FileListGen` will produce a `file list` file that `BatchPredictionGen` will refer to perform batch prediction on Vertex AI\n",
        "- `file list` format can be found [here](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions)\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - GCS path where the raw files are\n",
        "  - GCS path where the `file list` file will be \n",
        "- output\n",
        "  - GCS path where the `file list` file is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nit2eUyQx-Zo"
      },
      "source": [
        "#### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwlMeqr8qKTE"
      },
      "source": [
        "GOOGLE_CLOUD_PROJECT = 'central-hangar-321813'    #@param {type:\"string\"}\n",
        "GOOGLE_CLOUD_REGION = 'us-central1'             #@param {type:\"string\"}\n",
        "\n",
        "MODEL_NAME = 'resnet_cifar_latest' #@param {type:\"string\"}\n",
        "\n",
        "TEST_FILENAME = 'test-images.txt' #@param {type:\"string\"}\n",
        "TEST_GCS_BUCKET = 'batch-prediction-collection' #@param {type:\"string\"}\n",
        "TEST_GCS_PREFIX = '' #@param {type: \"string\"}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObPLaHx9pm6M"
      },
      "source": [
        "from google.cloud import storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKGOgj3KpnwV"
      },
      "source": [
        "client = storage.Client(project=GOOGLE_CLOUD_PROJECT)\n",
        "bucket = client.get_bucket(TEST_GCS_BUCKET)\n",
        "blobs = bucket.list_blobs(prefix=TEST_GCS_PREFIX)\n",
        "\n",
        "f = open(TEST_FILENAME, \"w\")\n",
        "\n",
        "for blob in blobs:\n",
        "  if blob.name.split('.')[-1] == \"jpg\":\n",
        "    if TEST_GCS_PREFIX != '':\n",
        "      TEST_GCS_PREFIX = f'/{TEST_GCS_PREFIX}'\n",
        "    line = f'{TEST_GCS_BUCKET}{TEST_GCS_PREFIX}/{blob.name}\\n'\n",
        "\n",
        "    f.write(line)\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMo28z34qaHF",
        "outputId": "40fe46af-601b-4eb9-c605-10c3719c4e38"
      },
      "source": [
        "!cat {TEST_FILENAME}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch-prediction-vertexai/airplane_0000.jpg\n",
            "batch-prediction-vertexai/automobile_0000.jpg\n",
            "batch-prediction-vertexai/bird_0000.jpg\n",
            "batch-prediction-vertexai/cat_0000.jpg\n",
            "batch-prediction-vertexai/deer_0000.jpg\n",
            "batch-prediction-vertexai/dog_0000.jpg\n",
            "batch-prediction-vertexai/frog_0000.jpg\n",
            "batch-prediction-vertexai/horse_0000.jpg\n",
            "batch-prediction-vertexai/ship_0000.jpg\n",
            "batch-prediction-vertexai/truck_0000.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Dgp0KWwWwn"
      },
      "source": [
        "blob = bucket.blob(f'test/{TEST_FILENAME}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hVk2ma4waMd"
      },
      "source": [
        "blob.upload_from_filename(TEST_FILENAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRXtyTYxx6Er"
      },
      "source": [
        "#### Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5z336WGoT0q"
      },
      "source": [
        "_file_list_gen_module_file = 'file_list_gen.py'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s_YUXiFobVr",
        "outputId": "e3b2eed3-acf2-4c97-c5cf-39860987022a"
      },
      "source": [
        "%%writefile {_file_list_gen_module_file}\n",
        "\n",
        "import tfx\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "from tfx.dsl.component.experimental.annotations import OutputDict\n",
        "from tfx.types.standard_artifacts import String\n",
        "from google.cloud import storage\n",
        "from absl import logging\n",
        "\n",
        "@component\n",
        "def FileListGen(\n",
        "    outpath: OutputArtifact[String],\n",
        "    project: Parameter[str],\n",
        "    gcs_source_bucket: Parameter[str],\n",
        "    gcs_source_prefix: Parameter[str] = '',\n",
        "    output_filename: Parameter[str] = 'test-images.txt'\n",
        "):\n",
        "  logging.info('FileListGen started')\n",
        "\n",
        "  client = storage.Client(project=project)\n",
        "  bucket = client.get_bucket(gcs_source_bucket)\n",
        "  blobs = bucket.list_blobs(prefix=gcs_source_prefix)\n",
        "  logging.info('Successfully retrieve the file(jpg) list from GCS path')\n",
        "\n",
        "  f = open(output_filename, 'w')\n",
        "  for blob in blobs:\n",
        "    if blob.name.split('.')[-1] == 'jpg':\n",
        "      prefix = ''\n",
        "      if gcs_source_prefix != '':\n",
        "        prefix = f'/{gcs_source_prefix}'\n",
        "      line = f'{gcs_source_bucket}{prefix}/{blob.name}\\n'\n",
        "      f.write(line)\n",
        "  f.close()\n",
        "  logging.info(f'Successfully created the file list file({output_filename}) in local storage')\n",
        "\n",
        "  prefix = ''\n",
        "  if gcs_source_prefix != '':\n",
        "    prefix = f'{gcs_source_prefix}/'\n",
        "  blob = bucket.blob(f'{prefix}{output_filename}')\n",
        "  blob.upload_from_filename(output_filename)\n",
        "  logging.info(f'Successfully uploaded the file list ({prefix}{output_filename})')\n",
        "\n",
        "  outpath.value = gcs_source_bucket + '/' + prefix + output_filename"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing file_list_gen.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ3mFDaCLgUh"
      },
      "source": [
        "### BatchPredictionGen Component\n",
        "- Behaviour of `BatchPredictionGen` is similar to TFX standard component `BulkInferrer`.\n",
        "- The only difference is we don't need `Model` artifact from `Trainer` but just `model ID` that can be found in `Vertex AI Model` registry.\n",
        "- Predicted results will be fed into the `PerformanceEvaluator` component.\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - GCS path where the TFRecord file is\n",
        "  - model id from Vertex AI Model\n",
        "- output\n",
        "  - predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4n7iyaVjyzB"
      },
      "source": [
        "#### Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2xEc3ZVzjNe"
      },
      "source": [
        "_batch_pred_module_file = 'batch_prediction_vertex.py'"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuknFT7hj0mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a0b314-8489-4a35-eedf-a0036a35dd05"
      },
      "source": [
        "%%writefile {_batch_pred_module_file}\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "from tfx.dsl.component.experimental.annotations import Parameter, InputArtifact\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types import artifact_utils\n",
        "from tfx.types.standard_artifacts import String\n",
        "import google.cloud.aiplatform as vertex_ai\n",
        "\n",
        "from typing import Union, Sequence\n",
        "from absl import logging\n",
        "\n",
        "@component\n",
        "def bulk_inferer_vertex(\n",
        "    project: Parameter[str],\n",
        "    location: Parameter[str],\n",
        "    model_resource_name: Parameter[str],\n",
        "    job_display_name: Parameter[str],\n",
        "    gcs_source: InputArtifact[String],\n",
        "    gcs_destination: Parameter[str],\n",
        "    instances_format: Parameter[str] = \"file-list\",\n",
        "    machine_type: Parameter[str] = \"n1-standard-2\",\n",
        "    accelerator_count: Parameter[int] = None,\n",
        "    accelerator_type: Parameter[str] = None,\n",
        "    starting_replica_count: Parameter[int] = 1,\n",
        "    max_replica_count: Parameter[int] = 1,\n",
        "):\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Read GCS Source (gcs_source contains the full path of GCS object)\n",
        "    # 1-1. get bucketname from gcs_source\n",
        "    gcs_source_uri = gcs_source.uri.split('//')[1:][0].split('/')\n",
        "    bucketname = gcs_source_uri[0]\n",
        "    bucket = storage_client.get_bucket(bucketname)\n",
        "    logging.info(f'bucketname: {bucketname}')\n",
        "\n",
        "    # 1-2. get object path without the bucketname\n",
        "    objectpath = '/'.join(gcs_source_uri[1:])\n",
        "\n",
        "    # 1-3. read the object to get value set by OutputArtifact from FileListGen\n",
        "    blob = bucket.blob(objectpath)\n",
        "    logging.info(f'objectpath: {objectpath}')\n",
        "\n",
        "    gcs_source = f'gs://{blob.download_as_text()}'\n",
        "    \n",
        "    # Get Model\n",
        "    vertex_ai.init(project=project, location=location)\n",
        "\n",
        "    model = vertex_ai.Model.list(\n",
        "        filter=f\"display_name={model_resource_name}\", order_by=\"update_time\"\n",
        "    )[-1]\n",
        "\n",
        "    # Batch Predictions\n",
        "    logging.info(\"Starting batch prediction job.\")\n",
        "    logging.info(f\"GCS path where file list is: {gcs_source}\")\n",
        "\n",
        "    batch_prediction_job = model.batch_predict(\n",
        "        job_display_name=job_display_name,\n",
        "        instances_format=instances_format,\n",
        "        gcs_source=gcs_source,\n",
        "        gcs_destination_prefix=gcs_destination,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        accelerator_type=accelerator_type,\n",
        "        starting_replica_count=starting_replica_count,\n",
        "        max_replica_count=max_replica_count,\n",
        "        sync=True,\n",
        "    )\n",
        "\n",
        "    logging.info(batch_prediction_job.display_name)\n",
        "    logging.info(batch_prediction_job.resource_name)\n",
        "    logging.info(batch_prediction_job.state)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting batch_prediction_vertex.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjz3oQvTLxPG"
      },
      "source": [
        "### **PerformanceEvaluator Component**\n",
        "- Calculate any performance metrics \n",
        "- Outputs if the model performance is above or below the given threshold\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - predictions\n",
        "  - threshold\n",
        "- output\n",
        "  - `True` or `False` by the threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N79u9i-tL5Jm"
      },
      "source": [
        "### PipelineTrigger Component\n",
        "- Trigger the training pipeline based on the `True` or `False` value by the threshold\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - `True` or `False` by threshold\n",
        "  - pipeline name to be triggered\n",
        "  - GCS path where the pipeline spec is \n",
        "  - GCP project ID\n",
        "  - GCP region\n",
        "- output\n",
        "  - None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQNNM_Y5DNI"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnsPuwUNL4R4"
      },
      "source": [
        "GOOGLE_CLOUD_PROJECT = 'central-hangar-321813'        #@param {type:\"string\"}\n",
        "GOOGLE_CLOUD_REGION = 'us-central1'                   #@param {type:\"string\"}\n",
        "GCS_BUCKET_NAME = 'cifar10-experimental-batch-csp'    #@param {type:\"string\"}\n",
        "\n",
        "TEST_FILENAME = 'test-images.txt' #@param {type:\"string\"}\n",
        "TEST_GCS_BUCKET = 'batch-prediction-collection' #@param {type:\"string\"}\n",
        "MODEL_RESOURCE_NAME = 'resnet_cifar_latest' #@param {type: \"string\"}\n",
        "TEST_GCS_PREFIX = '' #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
        "    from absl import logging\n",
        "    logging.error('Please set all required parameters.')"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOljVxpi5Xo1",
        "outputId": "08d65f53-89a8-44e0-d8a7-ac3445856167"
      },
      "source": [
        "PIPELINE_NAME = 'continuous-adaptation-for-data-changes-batch'\n",
        "\n",
        "# Path to various pipeline artifact.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIPELINE_ROOT: gs://cifar10-experimental-batch-csp/pipeline_root/continuous-adaptation-for-data-changes-batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eic7WtdY5iTz"
      },
      "source": [
        "!mkdir -p ./custom_components\n",
        "!touch ./custom_components/__init__.py\n",
        "!cp -r {_file_list_gen_module_file} {_batch_pred_module_file} custom_components"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rllJsXU9JwUZ",
        "outputId": "b94964ec-ddce-4622-c6c7-933384f47f29"
      },
      "source": [
        "!ls -lh custom_components"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "-rw-r--r-- 1 root root 2.5K Sep 26 07:37 batch_prediction_vertex.py\n",
            "-rw-r--r-- 1 root root 1.6K Sep 26 07:37 file_list_gen.py\n",
            "-rw-r--r-- 1 root root    0 Sep 26 07:37 __init__.py\n",
            "drwxr-xr-x 2 root root 4.0K Sep 26 06:34 __pycache__\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf-5AjJQ5w9Z",
        "outputId": "8432bade-01d9-4b28-8a90-6d49cff34755"
      },
      "source": [
        "DISPLAY_NAME = \"batch-predictions-pipeline\"\n",
        "VERSION = \"tfx-1-2-0-14\"\n",
        "TFX_IMAGE_URI = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{DISPLAY_NAME}:{VERSION}\"\n",
        "print(f\"URI of the custom image: {TFX_IMAGE_URI}\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URI of the custom image: gcr.io/central-hangar-321813/batch-predictions-pipeline:tfx-1-2-0-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvv4-FIG58P9",
        "outputId": "f12314a9-b57c-464b-ff12-a1cbc69c7470"
      },
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
        "RUN mkdir -p custom_components\n",
        "COPY custom_components/* ./custom_components/\n",
        "RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I86iu9-76DQi"
      },
      "source": [
        "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1OrDsPgKrfS"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDNAaIjB6HWy"
      },
      "source": [
        "import tfx\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from custom_components.file_list_gen import FileListGen\n",
        "from custom_components.batch_prediction_vertex import bulk_inferer_vertex\n",
        "\n",
        "\n",
        "def _create_pipeline(\n",
        "    pipeline_name: str,\n",
        "    pipeline_root: str,\n",
        "    data_gcs_bucket: str,\n",
        "    data_gcs_prefix: str,\n",
        "    batch_job_gcs: str,\n",
        "    job_display_name: str,\n",
        "    model_resource_name: str,\n",
        "    project_id: str,\n",
        "    region: str,\n",
        ") -> Pipeline :\n",
        "  \n",
        "  filelist_gen = FileListGen(\n",
        "      project = project_id, \n",
        "      gcs_source_bucket = data_gcs_bucket,\n",
        "      gcs_source_prefix = data_gcs_prefix,\n",
        "  ).with_id(\"filelist_gen\")\n",
        "\n",
        "  batch_pred_component = bulk_inferer_vertex(\n",
        "      project=project_id,\n",
        "      location=region,\n",
        "      job_display_name=job_display_name,\n",
        "      model_resource_name=model_resource_name,\n",
        "      gcs_source=filelist_gen.outputs['outpath'],\n",
        "      gcs_destination=f'gs://{batch_job_gcs}/results/'\n",
        "  ).with_id(\"bulk_inferer_vertex\")\n",
        "  batch_pred_component.add_upstream_node(filelist_gen)\n",
        "\n",
        "  components = [\n",
        "    filelist_gen,\n",
        "    batch_pred_component\n",
        "  ]\n",
        "\n",
        "  return Pipeline(\n",
        "      pipeline_name=pipeline_name, \n",
        "      pipeline_root=pipeline_root,\n",
        "      components=components\n",
        "  )"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi3BobZw7JiS"
      },
      "source": [
        "import os\n",
        "import tfx\n",
        "from tfx.orchestration.kubeflow.v2.kubeflow_v2_dag_runner import KubeflowV2DagRunner\n",
        "from tfx.orchestration.kubeflow.v2.kubeflow_v2_dag_runner import KubeflowV2DagRunnerConfig\n",
        "\n",
        "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
        "\n",
        "# Important: We need to pass the custom Docker image URI to the\n",
        "# `KubeflowV2DagRunnerConfig` to take effect.\n",
        "runner = KubeflowV2DagRunner(\n",
        "    config=KubeflowV2DagRunnerConfig(default_image=TFX_IMAGE_URI),\n",
        "    output_filename=PIPELINE_DEFINITION_FILE)\n",
        "\n",
        "_ = runner.run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        data_gcs_bucket=TEST_GCS_BUCKET,\n",
        "        data_gcs_prefix=TEST_GCS_PREFIX,\n",
        "        batch_job_gcs=GCS_BUCKET_NAME,\n",
        "        job_display_name=f\"{MODEL_RESOURCE_NAME}_{TIMESTAMP}\",\n",
        "        project_id=GOOGLE_CLOUD_PROJECT,\n",
        "        region=GOOGLE_CLOUD_REGION,\n",
        "        model_resource_name=MODEL_RESOURCE_NAME\n",
        "    )\n",
        ")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "M6ofDcYP7ogx",
        "outputId": "6485730d-a02b-43aa-cf18-fe109056a5e0"
      },
      "source": [
        "from kfp.v2.google import client\n",
        "\n",
        "pipelines_client = client.AIPlatformClient(\n",
        "    project_id=GOOGLE_CLOUD_PROJECT,\n",
        "    region=GOOGLE_CLOUD_REGION,\n",
        ")\n",
        "\n",
        "_ = pipelines_client.create_run_from_job_spec(PIPELINE_DEFINITION_FILE, enable_caching=False)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/continuous-adaptation-for-data-changes-batch-20210926070628?project=central-hangar-321813\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3BgesEuiuJg"
      },
      "source": [
        "![img](https://i.ibb.co/521KpYm/Screen-Shot-2021-09-19-at-12-33-18-AM.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okNlX9toiu9A"
      },
      "source": [
        ""
      ],
      "execution_count": 88,
      "outputs": []
    }
  ]
}