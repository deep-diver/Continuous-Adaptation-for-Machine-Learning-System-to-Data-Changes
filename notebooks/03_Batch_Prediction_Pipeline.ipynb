{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Batch_Prediction_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/03_Batch_Prediction_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqBUdDXK2IF"
      },
      "source": [
        "## Batch Prediction Pipeline\n",
        "\n",
        "The main purpose of this notebook is to build KFP pipeline doing the following steps\n",
        "\n",
        "1. Create an batch request input file (file list format) based on the files uploaded to a GCS bucket\n",
        "2. Run Batch Prediction on the trained model obtained from [02_TFX_Training_Pipeline.ipynb](https://github.com/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/02_TFX_Training_Pipeline.ipynb)\n",
        "3. Measure the batch prediction model performance in terms of accuracy\n",
        "4. If model **performance < threshold**\n",
        "  - Copy the testing images to the original(previous) dataset\n",
        "  - Trigger the TFX training pipeline with original data + newly added data\n",
        "\n",
        "The functional test for batch prediction is shown in a separate notebook, [98_Batch_Prediction_Test.ipynb](https://github.com/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/98_Batch_Prediction_Test.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuQerdmAK38q"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNC8roxxK3j5"
      },
      "source": [
        "!pip install fastdot\n",
        "!pip install tfx==1.2.0\n",
        "!pip install kfp==1.6.1\n",
        "!pip install -q --upgrade google-cloud-aiplatform\n",
        "!pip install -q --upgrade google-cloud-storage"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuCgpQiwchZZ"
      },
      "source": [
        "### ***Restart runtime.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UKtYc2IK2II"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd9CvkmRLFdu"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omrlUvFbLXsv"
      },
      "source": [
        "## Custom TFX Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "7zMGdk-lM7QX",
        "cellView": "form",
        "outputId": "29196151-66d1-458a-b15a-d370f02115f5"
      },
      "source": [
        "# @title\n",
        "from fastdot.core import *\n",
        "\n",
        "tfx_components = [\n",
        "    \"FileListGen\",\n",
        "    \"BatchPredictionGen\",\n",
        "    \"PerformanceEvaluator\",\n",
        "    \"PipelineTrigger\",\n",
        "]\n",
        "block = \"TFX Component Workflow\"\n",
        "\n",
        "g = graph_items(seq_cluster(tfx_components, block))\n",
        "g"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pydot.Dot at 0x7f3fb324bc10>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"613pt\" height=\"99pt\"\n viewBox=\"0.00 0.00 613.00 99.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 95)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-95 609,-95 609,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_n77046dadb7d54a0fa0401112b9d9f795</title>\n<g id=\"a_clust1\"><a xlink:title=\"TFX Component Workflow\">\n<path fill=\"#555555\" fill-opacity=\"0.133333\" stroke=\"#000000\" d=\"M20,-8C20,-8 585,-8 585,-8 591,-8 597,-14 597,-20 597,-20 597,-71 597,-71 597,-77 591,-83 585,-83 585,-83 20,-83 20,-83 14,-83 8,-77 8,-71 8,-71 8,-20 8,-20 8,-14 14,-8 20,-8\"/>\n<text text-anchor=\"middle\" x=\"302.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TFX Component Workflow</text>\n</a>\n</g>\n</g>\n<!-- n2367fe71f9ae455084070cb0ff77a788 -->\n<g id=\"node1\" class=\"node\">\n<title>n2367fe71f9ae455084070cb0ff77a788</title>\n<g id=\"a_node1\"><a xlink:title=\"FileListGen\">\n<path fill=\"#40ecd4\" fill-opacity=\"0.403922\" stroke=\"#000000\" d=\"M88,-52C88,-52 28,-52 28,-52 22,-52 16,-46 16,-40 16,-40 16,-28 16,-28 16,-22 22,-16 28,-16 28,-16 88,-16 88,-16 94,-16 100,-22 100,-28 100,-28 100,-40 100,-40 100,-46 94,-52 88,-52\"/>\n<text text-anchor=\"middle\" x=\"58\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FileListGen</text>\n</a>\n</g>\n</g>\n<!-- n8db514705c0145efbea4b30c20ab4338 -->\n<g id=\"node2\" class=\"node\">\n<title>n8db514705c0145efbea4b30c20ab4338</title>\n<g id=\"a_node2\"><a xlink:title=\"BatchPredictionGen\">\n<path fill=\"#ff4d27\" fill-opacity=\"0.678431\" stroke=\"#000000\" d=\"M256,-52C256,-52 148,-52 148,-52 142,-52 136,-46 136,-40 136,-40 136,-28 136,-28 136,-22 142,-16 148,-16 148,-16 256,-16 256,-16 262,-16 268,-22 268,-28 268,-28 268,-40 268,-40 268,-46 262,-52 256,-52\"/>\n<text text-anchor=\"middle\" x=\"202\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">BatchPredictionGen</text>\n</a>\n</g>\n</g>\n<!-- n2367fe71f9ae455084070cb0ff77a788&#45;&gt;n8db514705c0145efbea4b30c20ab4338 -->\n<g id=\"edge1\" class=\"edge\">\n<title>n2367fe71f9ae455084070cb0ff77a788&#45;&gt;n8db514705c0145efbea4b30c20ab4338</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M100.0379,-34C108.0929,-34 116.7569,-34 125.5097,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.7944,-37.5001 135.7944,-34 125.7943,-30.5001 125.7944,-37.5001\"/>\n</g>\n<!-- n2aa47a2d0a4b43bcb7d9102b72d7d90b -->\n<g id=\"node3\" class=\"node\">\n<title>n2aa47a2d0a4b43bcb7d9102b72d7d90b</title>\n<g id=\"a_node3\"><a xlink:title=\"PerformanceEvaluator\">\n<path fill=\"#feb562\" fill-opacity=\"0.454902\" stroke=\"#000000\" d=\"M436,-52C436,-52 316,-52 316,-52 310,-52 304,-46 304,-40 304,-40 304,-28 304,-28 304,-22 310,-16 316,-16 316,-16 436,-16 436,-16 442,-16 448,-22 448,-28 448,-28 448,-40 448,-40 448,-46 442,-52 436,-52\"/>\n<text text-anchor=\"middle\" x=\"376\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PerformanceEvaluator</text>\n</a>\n</g>\n</g>\n<!-- n8db514705c0145efbea4b30c20ab4338&#45;&gt;n2aa47a2d0a4b43bcb7d9102b72d7d90b -->\n<g id=\"edge2\" class=\"edge\">\n<title>n8db514705c0145efbea4b30c20ab4338&#45;&gt;n2aa47a2d0a4b43bcb7d9102b72d7d90b</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M268.2783,-34C276.6308,-34 285.2538,-34 293.8144,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"293.8153,-37.5001 303.8153,-34 293.8153,-30.5001 293.8153,-37.5001\"/>\n</g>\n<!-- n30d03b2407924bb09e6192fb36f663a2 -->\n<g id=\"node4\" class=\"node\">\n<title>n30d03b2407924bb09e6192fb36f663a2</title>\n<g id=\"a_node4\"><a xlink:title=\"PipelineTrigger\">\n<path fill=\"#ff351b\" fill-opacity=\"0.145098\" stroke=\"#000000\" d=\"M577,-52C577,-52 496,-52 496,-52 490,-52 484,-46 484,-40 484,-40 484,-28 484,-28 484,-22 490,-16 496,-16 496,-16 577,-16 577,-16 583,-16 589,-22 589,-28 589,-28 589,-40 589,-40 589,-46 583,-52 577,-52\"/>\n<text text-anchor=\"middle\" x=\"536.5\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PipelineTrigger</text>\n</a>\n</g>\n</g>\n<!-- n2aa47a2d0a4b43bcb7d9102b72d7d90b&#45;&gt;n30d03b2407924bb09e6192fb36f663a2 -->\n<g id=\"edge3\" class=\"edge\">\n<title>n2aa47a2d0a4b43bcb7d9102b72d7d90b&#45;&gt;n30d03b2407924bb09e6192fb36f663a2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M448.2681,-34C456.7862,-34 465.4254,-34 473.7968,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"473.8491,-37.5001 483.8491,-34 473.849,-30.5001 473.8491,-37.5001\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN4wUPP4Laqk"
      },
      "source": [
        "### **FileListGen Component**\n",
        "\n",
        "- `FileListGen` will produce a `file list` file that `BatchPredictionGen` will refer to perform batch prediction on Vertex AI\n",
        "- `file list` format can be found [here](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions)\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - GCS path where the raw files are\n",
        "  - GCS path where the `file list` file will be \n",
        "- output\n",
        "  - GCS path where the `file list` file is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5z336WGoT0q"
      },
      "source": [
        "_file_list_gen_module_file = 'file_list_gen.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s_YUXiFobVr",
        "outputId": "27a3d39e-20a1-4818-9de3-e13dbb96cd93"
      },
      "source": [
        "%%writefile {_file_list_gen_module_file}\n",
        "\n",
        "import tfx\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "from tfx.types.standard_artifacts import String\n",
        "from google.cloud import storage\n",
        "from absl import logging\n",
        "\n",
        "\n",
        "@component\n",
        "def FileListGen(\n",
        "    outpath: OutputArtifact[String],\n",
        "    project: Parameter[str],\n",
        "    gcs_source_bucket: Parameter[str],\n",
        "    gcs_source_prefix: Parameter[str] = \"\",\n",
        "    output_filename: Parameter[str] = \"test-images.txt\",\n",
        "):\n",
        "    logging.info(\"FileListGen started\")\n",
        "\n",
        "    client = storage.Client(project=project)\n",
        "    bucket = client.get_bucket(gcs_source_bucket)\n",
        "    blobs = bucket.list_blobs(prefix=gcs_source_prefix)\n",
        "    logging.info(\"Successfully retrieve the file(jpg) list from GCS path\")\n",
        "\n",
        "    f = open(output_filename, \"w\")\n",
        "    for blob in blobs:\n",
        "        if blob.name.split(\".\")[-1] == \"jpg\":\n",
        "            prefix = \"\"\n",
        "            if gcs_source_prefix != \"\":\n",
        "                prefix = f\"/{gcs_source_prefix}\"\n",
        "            line = f\"gs://{gcs_source_bucket}{prefix}/{blob.name}\\n\"\n",
        "            f.write(line)\n",
        "    f.close()\n",
        "    logging.info(\n",
        "        f\"Successfully created the file list file({output_filename}) in local storage\"\n",
        "    )\n",
        "\n",
        "    prefix = \"\"\n",
        "    if gcs_source_prefix != \"\":\n",
        "        prefix = f\"{gcs_source_prefix}/\"\n",
        "    blob = bucket.blob(f\"{prefix}{output_filename}\")\n",
        "    blob.upload_from_filename(output_filename)\n",
        "    logging.info(f\"Successfully uploaded the file list ({prefix}{output_filename})\")\n",
        "\n",
        "    outpath.value = gcs_source_bucket + \"/\" + prefix + output_filename"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing file_list_gen.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ3mFDaCLgUh"
      },
      "source": [
        "### BatchPredictionGen Component\n",
        "- Behaviour of `BatchPredictionGen` is similar to TFX standard component `BulkInferrer`.\n",
        "- The only difference is we don't need `Model` artifact from `Trainer` but just `model ID` that can be found in `Vertex AI Model` registry.\n",
        "- Predicted results will be fed into the `PerformanceEvaluator` component.\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - GCS path where the TFRecord file is\n",
        "  - model id from Vertex AI Model\n",
        "- output\n",
        "  - predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2xEc3ZVzjNe"
      },
      "source": [
        "_batch_pred_module_file = 'batch_prediction_vertex.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuknFT7hj0mu",
        "outputId": "5816963a-8886-4f4c-85bd-118df5e54176"
      },
      "source": [
        "%%writefile {_batch_pred_module_file}\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "from tfx.dsl.component.experimental.annotations import Parameter, InputArtifact\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types import artifact_utils\n",
        "from tfx.types.standard_artifacts import String\n",
        "import google.cloud.aiplatform as vertex_ai\n",
        "\n",
        "from typing import Union, Sequence\n",
        "from absl import logging\n",
        "\n",
        "\n",
        "@component\n",
        "def BatchPredictionGen(\n",
        "    gcs_source: InputArtifact[String],\n",
        "    project: Parameter[str],\n",
        "    location: Parameter[str],\n",
        "    model_resource_name: Parameter[str],\n",
        "    job_display_name: Parameter[str],\n",
        "    gcs_destination: Parameter[str],\n",
        "    instances_format: Parameter[str] = \"file-list\",\n",
        "    machine_type: Parameter[str] = \"n1-standard-2\",\n",
        "    accelerator_count: Parameter[int] = 0,\n",
        "    accelerator_type: Parameter[str] = None,\n",
        "    starting_replica_count: Parameter[int] = 1,\n",
        "    max_replica_count: Parameter[int] = 1,\n",
        "):\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Read GCS Source (gcs_source contains the full path of GCS object)\n",
        "    # 1-1. get bucketname from gcs_source\n",
        "    gcs_source_uri = gcs_source.uri.split(\"//\")[1:][0].split(\"/\")\n",
        "    bucketname = gcs_source_uri[0]\n",
        "    bucket = storage_client.get_bucket(bucketname)\n",
        "    logging.info(f\"bucketname: {bucketname}\")\n",
        "\n",
        "    # 1-2. get object path without the bucketname\n",
        "    objectpath = \"/\".join(gcs_source_uri[1:])\n",
        "\n",
        "    # 1-3. read the object to get value set by OutputArtifact from FileListGen\n",
        "    blob = bucket.blob(objectpath)\n",
        "    logging.info(f\"objectpath: {objectpath}\")\n",
        "\n",
        "    gcs_source = f\"gs://{blob.download_as_text()}\"\n",
        "\n",
        "    # Get Model\n",
        "    vertex_ai.init(project=project, location=location)\n",
        "\n",
        "    model = vertex_ai.Model.list(\n",
        "        filter=f\"display_name={model_resource_name}\", order_by=\"update_time\"\n",
        "    )[-1]\n",
        "\n",
        "    # Batch Predictions\n",
        "    logging.info(\"Starting batch prediction job.\")\n",
        "    logging.info(f\"GCS path where file list is: {gcs_source}\")\n",
        "\n",
        "    batch_prediction_job = model.batch_predict(\n",
        "        job_display_name=job_display_name,\n",
        "        instances_format=instances_format,\n",
        "        gcs_source=gcs_source,\n",
        "        gcs_destination_prefix=gcs_destination,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        accelerator_type=accelerator_type,\n",
        "        starting_replica_count=starting_replica_count,\n",
        "        max_replica_count=max_replica_count,\n",
        "        sync=True,\n",
        "    )\n",
        "\n",
        "    logging.info(batch_prediction_job.display_name)\n",
        "    logging.info(batch_prediction_job.resource_name)\n",
        "    logging.info(batch_prediction_job.state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting batch_prediction_vertex.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "bjz3oQvTLxPG"
      },
      "source": [
        "### **PerformanceEvaluator Component**\n",
        "- Calculate any performance metrics \n",
        "- Outputs if the model performance is above or below the given threshold\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - predictions\n",
        "  - threshold\n",
        "- output\n",
        "  - `True` or `False` by the threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mc4Sm0gF65F"
      },
      "source": [
        "_evaluator_module_file = 'batch_pred_evaluator.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv-UAqCHFvU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3345d7-b733-425a-cbed-882c43e57c82"
      },
      "source": [
        "%%writefile {_evaluator_module_file}\n",
        "\n",
        "# Reference: https://bit.ly/vertex-batch\n",
        "\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "\n",
        "from absl import logging\n",
        "import os\n",
        "import json\n",
        "\n",
        "@component\n",
        "def PerformanceEvaluator(\n",
        "    gcs_destination: Parameter[str],\n",
        "    local_directory: Parameter[str],\n",
        "    threshold: Parameter[float],\n",
        "    trigger_pipeline: OutputArtifact[Dataset],\n",
        "):\n",
        "    full_gcs_results_dir = f\"{gcs_destination}/{local_directory}\"\n",
        "\n",
        "    # Create missing directories.\n",
        "    os.makedirs(local_directory, exist_ok=True)\n",
        "\n",
        "    # Get the Cloud Storage paths for each result.\n",
        "    os.system(f\"gsutil -m cp -r {full_gcs_results_dir} {local_directory}\")\n",
        "\n",
        "    # Get most recently modified directory.\n",
        "    latest_directory = max(\n",
        "        [os.path.join(local_directory, d) for d in os.listdir(local_directory)],\n",
        "        key=os.path.getmtime,\n",
        "    )\n",
        "\n",
        "    # Get downloaded results in directory.\n",
        "    results_files = []\n",
        "    for dirpath, subdirs, files in os.walk(latest_directory):\n",
        "        for file in files:\n",
        "            if file.startswith(\"prediction.results\"):\n",
        "                results_files.append(os.path.join(dirpath, file))\n",
        "\n",
        "    # Consolidate all the results into a list.\n",
        "    results = []\n",
        "    for results_file in results_files:\n",
        "        # Download each result.\n",
        "        with open(results_file, \"r\") as file:\n",
        "            results.extend([json.loads(line) for line in file.readlines()])\n",
        "\n",
        "    # Calculate performance.\n",
        "    num_correct = 0\n",
        "\n",
        "    for result in results:\n",
        "        label = os.path.basename(result[\"instance\"]).split(\"_\")[0]\n",
        "        prediction = result[\"prediction\"][\"label\"]\n",
        "\n",
        "        if label == prediction:\n",
        "            num_correct = num_correct + 1\n",
        "\n",
        "    accuracy = num_correct / len(results)\n",
        "    logging.info(f\"Accuracy: {accuracy*100}%\")\n",
        "    trigger_pipeline.set_string_custom_property(\"result\", str(accuracy >= threshold))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing batch_pred_evaluator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "N79u9i-tL5Jm"
      },
      "source": [
        "### PipelineTrigger Component\n",
        "- Trigger the training pipeline based on the `True` or `False` value by the threshold\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - `True` or `False` by threshold\n",
        "  - pipeline name to be triggered\n",
        "  - GCS path where the pipeline spec is \n",
        "  - GCP project ID\n",
        "  - GCP region\n",
        "- output\n",
        "  - None"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWxscRQ03h9c"
      },
      "source": [
        "_pipeline_trigger_module_file = 'training_pipeline_trigger.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuuW5kfa3hfN"
      },
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "from kfp.v2.google.client import AIPlatformClient\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "\n",
        "from absl import logging\n",
        "\n",
        "@component\n",
        "def PipelineTrigger(\n",
        "\tpipeline_spec_path: Parameter[str],\n",
        "\tproject_id: Parameter[str],\n",
        "\tregion: Parameter[str],\n",
        "):\t\n",
        "\t# Check if the pipeline spec exists.\n",
        "\tstorage_client = storage.Client()\n",
        "\n",
        "\tpath_parts = pipeline_spec_path.replace(\"gs://\", \"\").split(\"/\")\n",
        "\tbucket_name = path_parts[0]\n",
        "\tblob_name = \"/\".join(path_parts[1:])\n",
        "\n",
        "\tbucket = storage_client.bucket(bucket_name)\n",
        "\tblob = storage.Blob(bucket=bucket, name=blob_name)\n",
        "\n",
        "\tif not blob.exists(storage_client):\n",
        "\t  raise ValueError(f\"{pipeline_spec_path} does not exist.\")\n",
        "\n",
        "\t# Initialize Vertex AI API client and submit for pipeline execution.\n",
        "\tapi_client = AIPlatformClient(project_id=project_id, region=region)\n",
        "\n",
        "\tresponse = api_client.create_run_from_job_spec(\n",
        "\t  job_spec_path=gcs_pipeline_file_location,\n",
        "\t  enable_caching=True,\n",
        "\t)\n",
        "\n",
        "\tlogging.info(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQNNM_Y5DNI"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjpDLeePZQR4"
      },
      "source": [
        "### Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnsPuwUNL4R4"
      },
      "source": [
        "# This bucket will be responsible for storing the pipeline related artifacts.\n",
        "GOOGLE_CLOUD_PROJECT = 'fast-ai-exploration'    #@param {type:\"string\"}\n",
        "GOOGLE_CLOUD_REGION = 'us-central1'      \n",
        "\n",
        "GCS_BUCKET_NAME = 'cifar10-experimental-sayakpaul'    #@param {type:\"string\"}\n",
        "\n",
        "MODEL_RESOURCE_NAME = 'resnet_cifar_latest' #@param {type: \"string\"}\n",
        "\n",
        "TEST_FILENAME = 'test-images.txt' #@param {type:\"string\"}\n",
        "TEST_GCS_BUCKET = 'batch-prediction-vertexai' #@param {type:\"string\"}\n",
        "TEST_GCS_PREFIX = '' #@param {type: \"string\"}\n",
        "\n",
        "TRAINING_PIPELINE_SPEC = 'gs://cifar10-experimental-csp2/pipeline_root/continuous-adaptation-for-data-changes/continuous-adaptation-for-data-changes_pipeline.json' #@param {type: \"string\"}\n",
        "\n",
        "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
        "    from absl import logging\n",
        "    logging.error('Please set all required parameters.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOljVxpi5Xo1",
        "outputId": "7431c46a-c9b1-40d2-d50c-b015a2aad327"
      },
      "source": [
        "PIPELINE_NAME = 'continuous-adaptation-for-data-changes-batch'\n",
        "\n",
        "# Path to various pipeline artifact.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIPELINE_ROOT: gs://cifar10-experimental-sayakpaul/pipeline_root/continuous-adaptation-for-data-changes-batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzy9y2EGZQR5"
      },
      "source": [
        "### Custom Docker image to run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eic7WtdY5iTz"
      },
      "source": [
        "!mkdir -p ./custom_components\n",
        "!touch ./custom_components/__init__.py\n",
        "!cp -r {_file_list_gen_module_file} {_batch_pred_module_file} {_evaluator_module_file} custom_components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rllJsXU9JwUZ",
        "outputId": "55646385-30e6-48a3-91a2-3e73b2882677"
      },
      "source": [
        "!ls -lh custom_components"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12K\n",
            "-rw-r--r-- 1 root root 2.0K Sep 27 02:37 batch_pred_evaluator.py\n",
            "-rw-r--r-- 1 root root 2.5K Sep 27 02:37 batch_prediction_vertex.py\n",
            "-rw-r--r-- 1 root root 1.6K Sep 27 02:37 file_list_gen.py\n",
            "-rw-r--r-- 1 root root    0 Sep 27 02:37 __init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf-5AjJQ5w9Z",
        "outputId": "d6fadf10-ee13-47b0-9585-327ff70be84b"
      },
      "source": [
        "DISPLAY_NAME = \"batch-predictions-pipeline\"\n",
        "VERSION = \"tfx-1-2-0-17\"\n",
        "TFX_IMAGE_URI = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{DISPLAY_NAME}:{VERSION}\"\n",
        "print(f\"URI of the custom image: {TFX_IMAGE_URI}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URI of the custom image: gcr.io/fast-ai-exploration/batch-predictions-pipeline:tfx-1-2-0-17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvv4-FIG58P9",
        "outputId": "10b4c6dd-a2eb-4fcd-e4a6-1415b9c98baf"
      },
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
        "RUN mkdir -p custom_components\n",
        "COPY custom_components/* ./custom_components/\n",
        "RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I86iu9-76DQi"
      },
      "source": [
        "%%capture\n",
        "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMl_qx32ZQR6"
      },
      "source": [
        "### Create the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1OrDsPgKrfS"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDNAaIjB6HWy"
      },
      "source": [
        "import tfx\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from custom_components.file_list_gen import FileListGen\n",
        "from custom_components.batch_prediction_vertex import BatchPredictionGen\n",
        "from custom_components.batch_pred_evaluator import PerformanceEvaluator\n",
        "from custom_components.training_pipeline_trigger import PipelineTrigger\n",
        "\n",
        "def _create_pipeline(\n",
        "    pipeline_name: str,\n",
        "    pipeline_root: str,\n",
        "    data_gcs_bucket: str,\n",
        "    data_gcs_prefix: str,\n",
        "    batch_job_gcs: str,\n",
        "    job_display_name: str,\n",
        "    model_resource_name: str,\n",
        "    project_id: str,\n",
        "    region: str,\n",
        "    threshold: float,\n",
        "    training_pipeline_spec: str,\n",
        ") -> Pipeline:\n",
        "\n",
        "    # Generate a file list for batch preditions. \n",
        "    # More details on the structure of this file here:\n",
        "    # https://bit.ly/3BzfHVu.\n",
        "    filelist_gen = FileListGen(\n",
        "        project=project_id,\n",
        "        gcs_source_bucket=data_gcs_bucket,\n",
        "        gcs_source_prefix=data_gcs_prefix,\n",
        "    ).with_id(\"filelist_gen\")\n",
        "\n",
        "    # Submit a batch prediction job.\n",
        "    batch_pred_component = BatchPredictionGen(\n",
        "        project=project_id,\n",
        "        location=region,\n",
        "        job_display_name=job_display_name,\n",
        "        model_resource_name=model_resource_name,\n",
        "        gcs_source=filelist_gen.outputs[\"outpath\"],\n",
        "        gcs_destination=f\"gs://{batch_job_gcs}/results/\",\n",
        "        accelerator_count=0,\n",
        "        accelerator_type=None,\n",
        "    ).with_id(\"bulk_inferer_vertex\")\n",
        "    batch_pred_component.add_upstream_node(filelist_gen)\n",
        "\n",
        "    # Evaluate the performance of the predictions. \n",
        "    # In a real-world project, this evaluation takes place\n",
        "    # separately, typically with the help of domain experts. \n",
        "    final_gcs_destination=f\"gs://{batch_job_gcs}/results/\"\n",
        "    evaluator = PerformanceEvaluator(\n",
        "        gcs_destination=f'gs://{final_gcs_destination.split(\"/\")[2]}',\n",
        "        local_directory=final_gcs_destination.split(\"/\")[-2],\n",
        "        threshold=threshold\n",
        "    ).with_id(\"batch_prediction_evaluator\")\n",
        "    evaluator.add_upstream_node(batch_pred_component)\n",
        "\n",
        "    trigger = PipelineTrigger(\n",
        "      pipeline_spec_path=training_pipeline_spec,\n",
        "      project_id=project_id,\n",
        "      region=region\n",
        "    ).with_id(\"training_pipeline_trigger\")\n",
        "    trigger.add_upstream_node(evaluator)\n",
        "\n",
        "    components = [filelist_gen, batch_pred_component, evaluator, trigger]\n",
        "\n",
        "    # If the evaluation score is below the threshold, trigger re-training.\n",
        "    re_train = evaluator.outputs[\"trigger_pipeline\"].get()[0].get_string_custom_property(\"result\")\n",
        "    if retrain:\n",
        "        # First, fetch the pipeline spec path.\n",
        "        pipeline_spec_path = os.path.join(pipeline_root, PIPELINE_DEFINITION_FILE)\n",
        "\n",
        "        # Then submit a training job to Vertex AI.\n",
        "        trainer = PipelineTrigger(pipeline_spec_path, project_id, region)\n",
        "        components.extend(trainer)\n",
        "\n",
        "    return Pipeline(\n",
        "        pipeline_name=pipeline_name, pipeline_root=pipeline_root, components=components\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE4A6ApPZQR7"
      },
      "source": [
        "### Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi3BobZw7JiS"
      },
      "source": [
        "import os\n",
        "import tfx\n",
        "from tfx.orchestration.kubeflow.v2.kubeflow_v2_dag_runner import KubeflowV2DagRunner\n",
        "from tfx.orchestration.kubeflow.v2.kubeflow_v2_dag_runner import (\n",
        "    KubeflowV2DagRunnerConfig,\n",
        ")\n",
        "\n",
        "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + \"_pipeline.json\"\n",
        "THRESHOLD = 0.9\n",
        "\n",
        "# Important: We need to pass the custom Docker image URI to the\n",
        "# `KubeflowV2DagRunnerConfig` to take effect.\n",
        "runner = KubeflowV2DagRunner(\n",
        "    config=KubeflowV2DagRunnerConfig(default_image=TFX_IMAGE_URI),\n",
        "    output_filename=PIPELINE_DEFINITION_FILE,\n",
        ")\n",
        "\n",
        "_ = runner.run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        data_gcs_bucket=TEST_GCS_BUCKET,\n",
        "        data_gcs_prefix=TEST_GCS_PREFIX,\n",
        "        batch_job_gcs=GCS_BUCKET_NAME,\n",
        "        job_display_name=f\"{MODEL_RESOURCE_NAME}_{TIMESTAMP}\",\n",
        "        project_id=GOOGLE_CLOUD_PROJECT,\n",
        "        region=GOOGLE_CLOUD_REGION,\n",
        "        model_resource_name=MODEL_RESOURCE_NAME,\n",
        "        threshold=THRESHOLD,\n",
        "        training_pipeline_spec=TRAINING_PIPELINE_SPEC,\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "M6ofDcYP7ogx",
        "outputId": "5601f7ff-de86-4c7a-ebf5-f67d363e880b"
      },
      "source": [
        "from kfp.v2.google import client\n",
        "\n",
        "pipelines_client = client.AIPlatformClient(\n",
        "    project_id=GOOGLE_CLOUD_PROJECT,\n",
        "    region=GOOGLE_CLOUD_REGION,\n",
        ")\n",
        "\n",
        "_ = pipelines_client.create_run_from_job_spec(PIPELINE_DEFINITION_FILE, enable_caching=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/continuous-adaptation-for-data-changes-batch-20210927035117?project=fast-ai-exploration\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okNlX9toiu9A"
      },
      "source": [
        "This is how the pipeline should look like:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.ibb.co/nDGS5Xk/Screenshot-2021-09-27-at-9-46-41-AM.png\" width=500></ing>"
      ]
    }
  ]
}