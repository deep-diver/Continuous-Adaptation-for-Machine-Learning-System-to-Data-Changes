{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Batch_Prediction_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/03_Batch_Prediction_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqBUdDXK2IF"
      },
      "source": [
        "## Batch Prediction Pipeline\n",
        "\n",
        "The main purpose of this notebook is to build KFP pipeline doing the following steps\n",
        "\n",
        "1. Create an batch request input file (file list format) based on the files uploaded to a GCS bucket\n",
        "2. Run Batch Prediction on the trained model obtained from [02_TFX_Training_Pipeline.ipynb](https://github.com/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/02_TFX_Training_Pipeline.ipynb)\n",
        "3. Measure the batch prediction model performance in terms of accuracy\n",
        "4. If model **performance < threshold**\n",
        "  - Copy the testing images to the original(previous) dataset\n",
        "  - Trigger the TFX training pipeline with original data + newly added data\n",
        "\n",
        "The functional test for batch prediction is shown in a separate notebook, [98_Batch_Prediction_Test.ipynb](https://github.com/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/98_Batch_Prediction_Test.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuQerdmAK38q"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNC8roxxK3j5"
      },
      "source": [
        "%%capture\n",
        "!pip install fastdot\n",
        "!pip install tfx==1.2.0\n",
        "!pip install kfp==1.6.1\n",
        "!pip install -q --upgrade google-cloud-aiplatform\n",
        "!pip install -q --upgrade google-cloud-storage"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuCgpQiwchZZ"
      },
      "source": [
        "### ***Restart runtime.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UKtYc2IK2II"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd9CvkmRLFdu"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omrlUvFbLXsv"
      },
      "source": [
        "## Custom TFX Components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "7zMGdk-lM7QX",
        "outputId": "fdce8d6b-8fa3-497f-f3cd-6a0b3a35df11"
      },
      "source": [
        "# @title\n",
        "from fastdot.core import *\n",
        "\n",
        "tfx_components = [\n",
        "    \"FileListGen\",\n",
        "    \"BatchPredictionGen\",\n",
        "    \"PerformanceEvaluator\",\n",
        "    \"SpanPreparator\",\n",
        "    \"PipelineTrigger\",\n",
        "]\n",
        "block = \"TFX Component Workflow\"\n",
        "\n",
        "g = graph_items(seq_cluster(tfx_components, block))\n",
        "g"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pydot.Dot at 0x7ff439c5f5d0>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"753pt\" height=\"99pt\"\n viewBox=\"0.00 0.00 753.00 99.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 95)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-95 749,-95 749,4 -4,4\"/>\n<g id=\"clust1\" class=\"cluster\">\n<title>cluster_nfd41977f519b4c7ca3a4ac0cf0cb9a56</title>\n<g id=\"a_clust1\"><a xlink:title=\"TFX Component Workflow\">\n<path fill=\"#555555\" fill-opacity=\"0.133333\" stroke=\"#000000\" d=\"M20,-8C20,-8 725,-8 725,-8 731,-8 737,-14 737,-20 737,-20 737,-71 737,-71 737,-77 731,-83 725,-83 725,-83 20,-83 20,-83 14,-83 8,-77 8,-71 8,-71 8,-20 8,-20 8,-14 14,-8 20,-8\"/>\n<text text-anchor=\"middle\" x=\"372.5\" y=\"-67.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">TFX Component Workflow</text>\n</a>\n</g>\n</g>\n<!-- n9bced685de7b4b9cb2d70f59b8b240c8 -->\n<g id=\"node1\" class=\"node\">\n<title>n9bced685de7b4b9cb2d70f59b8b240c8</title>\n<g id=\"a_node1\"><a xlink:title=\"FileListGen\">\n<path fill=\"#ff532a\" fill-opacity=\"0.741176\" stroke=\"#000000\" d=\"M88,-52C88,-52 28,-52 28,-52 22,-52 16,-46 16,-40 16,-40 16,-28 16,-28 16,-22 22,-16 28,-16 28,-16 88,-16 88,-16 94,-16 100,-22 100,-28 100,-28 100,-40 100,-40 100,-46 94,-52 88,-52\"/>\n<text text-anchor=\"middle\" x=\"58\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">FileListGen</text>\n</a>\n</g>\n</g>\n<!-- nbc584c4d9b98497aad6f447c73ed9349 -->\n<g id=\"node2\" class=\"node\">\n<title>nbc584c4d9b98497aad6f447c73ed9349</title>\n<g id=\"a_node2\"><a xlink:title=\"BatchPredictionGen\">\n<path fill=\"#583efd\" fill-opacity=\"0.176471\" stroke=\"#000000\" d=\"M256,-52C256,-52 148,-52 148,-52 142,-52 136,-46 136,-40 136,-40 136,-28 136,-28 136,-22 142,-16 148,-16 148,-16 256,-16 256,-16 262,-16 268,-22 268,-28 268,-28 268,-40 268,-40 268,-46 262,-52 256,-52\"/>\n<text text-anchor=\"middle\" x=\"202\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">BatchPredictionGen</text>\n</a>\n</g>\n</g>\n<!-- n9bced685de7b4b9cb2d70f59b8b240c8&#45;&gt;nbc584c4d9b98497aad6f447c73ed9349 -->\n<g id=\"edge1\" class=\"edge\">\n<title>n9bced685de7b4b9cb2d70f59b8b240c8&#45;&gt;nbc584c4d9b98497aad6f447c73ed9349</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M100.0379,-34C108.0929,-34 116.7569,-34 125.5097,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"125.7944,-37.5001 135.7944,-34 125.7943,-30.5001 125.7944,-37.5001\"/>\n</g>\n<!-- n15ae068ea9534b1689f91e2f3df95f4e -->\n<g id=\"node3\" class=\"node\">\n<title>n15ae068ea9534b1689f91e2f3df95f4e</title>\n<g id=\"a_node3\"><a xlink:title=\"PerformanceEvaluator\">\n<path fill=\"#2adddd\" fill-opacity=\"0.298039\" stroke=\"#000000\" d=\"M436,-52C436,-52 316,-52 316,-52 310,-52 304,-46 304,-40 304,-40 304,-28 304,-28 304,-22 310,-16 316,-16 316,-16 436,-16 436,-16 442,-16 448,-22 448,-28 448,-28 448,-40 448,-40 448,-46 442,-52 436,-52\"/>\n<text text-anchor=\"middle\" x=\"376\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PerformanceEvaluator</text>\n</a>\n</g>\n</g>\n<!-- nbc584c4d9b98497aad6f447c73ed9349&#45;&gt;n15ae068ea9534b1689f91e2f3df95f4e -->\n<g id=\"edge2\" class=\"edge\">\n<title>nbc584c4d9b98497aad6f447c73ed9349&#45;&gt;n15ae068ea9534b1689f91e2f3df95f4e</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M268.2783,-34C276.6308,-34 285.2538,-34 293.8144,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"293.8153,-37.5001 303.8153,-34 293.8153,-30.5001 293.8153,-37.5001\"/>\n</g>\n<!-- n4711f24c790d495e9799d9b26635083f -->\n<g id=\"node4\" class=\"node\">\n<title>n4711f24c790d495e9799d9b26635083f</title>\n<g id=\"a_node4\"><a xlink:title=\"SpanPreparator\">\n<path fill=\"#9efaa2\" fill-opacity=\"0.196078\" stroke=\"#000000\" d=\"M576,-52C576,-52 496,-52 496,-52 490,-52 484,-46 484,-40 484,-40 484,-28 484,-28 484,-22 490,-16 496,-16 496,-16 576,-16 576,-16 582,-16 588,-22 588,-28 588,-28 588,-40 588,-40 588,-46 582,-52 576,-52\"/>\n<text text-anchor=\"middle\" x=\"536\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">SpanPreparator</text>\n</a>\n</g>\n</g>\n<!-- n15ae068ea9534b1689f91e2f3df95f4e&#45;&gt;n4711f24c790d495e9799d9b26635083f -->\n<g id=\"edge3\" class=\"edge\">\n<title>n15ae068ea9534b1689f91e2f3df95f4e&#45;&gt;n4711f24c790d495e9799d9b26635083f</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M448.043,-34C456.6525,-34 465.3861,-34 473.8397,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"473.9853,-37.5001 483.9852,-34 473.9852,-30.5001 473.9853,-37.5001\"/>\n</g>\n<!-- n6d3de6cbc3774fd88587093cb9d776b2 -->\n<g id=\"node5\" class=\"node\">\n<title>n6d3de6cbc3774fd88587093cb9d776b2</title>\n<g id=\"a_node5\"><a xlink:title=\"PipelineTrigger\">\n<path fill=\"#9bfba5\" fill-opacity=\"0.705882\" stroke=\"#000000\" d=\"M717,-52C717,-52 636,-52 636,-52 630,-52 624,-46 624,-40 624,-40 624,-28 624,-28 624,-22 630,-16 636,-16 636,-16 717,-16 717,-16 723,-16 729,-22 729,-28 729,-28 729,-40 729,-40 729,-46 723,-52 717,-52\"/>\n<text text-anchor=\"middle\" x=\"676.5\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">PipelineTrigger</text>\n</a>\n</g>\n</g>\n<!-- n4711f24c790d495e9799d9b26635083f&#45;&gt;n6d3de6cbc3774fd88587093cb9d776b2 -->\n<g id=\"edge4\" class=\"edge\">\n<title>n4711f24c790d495e9799d9b26635083f&#45;&gt;n6d3de6cbc3774fd88587093cb9d776b2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M588.317,-34C596.5262,-34 605.0854,-34 613.5043,-34\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"613.6708,-37.5001 623.6707,-34 613.6707,-30.5001 613.6708,-37.5001\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN4wUPP4Laqk"
      },
      "source": [
        "### **FileListGen Component**\n",
        "\n",
        "- `FileListGen` will produce a `file list` file that `BatchPredictionGen` will refer to perform batch prediction on Vertex AI\n",
        "- `file list` format can be found [here](https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions)\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - GCS path where the raw files are\n",
        "  - GCS path where the `file list` file will be \n",
        "- output\n",
        "  - GCS path where the `file list` file is"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5z336WGoT0q"
      },
      "source": [
        "_file_list_gen_module_file = 'file_list_gen.py'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s_YUXiFobVr",
        "outputId": "c3097a50-3f3b-4415-c64f-f3a6f6302738"
      },
      "source": [
        "%%writefile {_file_list_gen_module_file}\n",
        "\n",
        "import tfx\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "from tfx.types.standard_artifacts import String\n",
        "from google.cloud import storage\n",
        "from absl import logging\n",
        "\n",
        "\n",
        "@component\n",
        "def FileListGen(\n",
        "    outpath: OutputArtifact[String],\n",
        "    project: Parameter[str],\n",
        "    gcs_source_bucket: Parameter[str],\n",
        "    gcs_source_prefix: Parameter[str] = \"\",\n",
        "    output_filename: Parameter[str] = \"test-images.txt\",\n",
        "):\n",
        "    logging.info(\"FileListGen started\")\n",
        "\n",
        "    client = storage.Client(project=project)\n",
        "    bucket = client.get_bucket(gcs_source_bucket)\n",
        "    blobs = bucket.list_blobs(prefix=gcs_source_prefix)\n",
        "    logging.info(\"Successfully retrieve the file(jpg) list from GCS path\")\n",
        "\n",
        "    f = open(output_filename, \"w\")\n",
        "    for blob in blobs:\n",
        "        if blob.name.split(\".\")[-1] == \"jpg\":\n",
        "            prefix = \"\"\n",
        "            if gcs_source_prefix != \"\":\n",
        "                prefix = f\"/{gcs_source_prefix}\"\n",
        "            line = f\"gs://{gcs_source_bucket}{prefix}/{blob.name}\\n\"\n",
        "            f.write(line)\n",
        "    f.close()\n",
        "    logging.info(\n",
        "        f\"Successfully created the file list file({output_filename}) in local storage\"\n",
        "    )\n",
        "\n",
        "    prefix = \"\"\n",
        "    if gcs_source_prefix != \"\":\n",
        "        prefix = f\"{gcs_source_prefix}/\"\n",
        "    blob = bucket.blob(f\"{prefix}{output_filename}\")\n",
        "    blob.upload_from_filename(output_filename)\n",
        "    logging.info(f\"Successfully uploaded the file list ({prefix}{output_filename})\")\n",
        "\n",
        "    outpath.value = gcs_source_bucket + \"/\" + prefix + output_filename"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing file_list_gen.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ3mFDaCLgUh"
      },
      "source": [
        "### BatchPredictionGen Component\n",
        "- Behaviour of `BatchPredictionGen` is similar to TFX standard component `BulkInferrer`.\n",
        "- The only difference is we don't need `Model` artifact from `Trainer` but just `model ID` that can be found in `Vertex AI Model` registry.\n",
        "- Predicted results will be fed into the `PerformanceEvaluator` component.\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - GCS path where the TFRecord file is\n",
        "  - model id from Vertex AI Model\n",
        "- output\n",
        "  - predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2xEc3ZVzjNe"
      },
      "source": [
        "_batch_pred_module_file = 'batch_prediction_vertex.py'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuknFT7hj0mu",
        "outputId": "ba764eac-a468-4eb9-b1d8-5ca623a513ca"
      },
      "source": [
        "%%writefile {_batch_pred_module_file}\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "from tfx.dsl.component.experimental.annotations import Parameter, InputArtifact\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types import artifact_utils\n",
        "from tfx.types.standard_artifacts import String\n",
        "import google.cloud.aiplatform as vertex_ai\n",
        "\n",
        "from typing import Union, Sequence\n",
        "from absl import logging\n",
        "\n",
        "\n",
        "@component\n",
        "def BatchPredictionGen(\n",
        "    gcs_source: InputArtifact[String],\n",
        "    project: Parameter[str],\n",
        "    location: Parameter[str],\n",
        "    model_resource_name: Parameter[str],\n",
        "    job_display_name: Parameter[str],\n",
        "    gcs_destination: Parameter[str],\n",
        "    instances_format: Parameter[str] = \"file-list\",\n",
        "    machine_type: Parameter[str] = \"n1-standard-2\",\n",
        "    accelerator_count: Parameter[int] = 0,\n",
        "    accelerator_type: Parameter[str] = None,\n",
        "    starting_replica_count: Parameter[int] = 1,\n",
        "    max_replica_count: Parameter[int] = 1,\n",
        "):\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Read GCS Source (gcs_source contains the full path of GCS object)\n",
        "    # 1-1. get bucketname from gcs_source\n",
        "    gcs_source_uri = gcs_source.uri.split(\"//\")[1:][0].split(\"/\")\n",
        "    bucketname = gcs_source_uri[0]\n",
        "    bucket = storage_client.get_bucket(bucketname)\n",
        "    logging.info(f\"bucketname: {bucketname}\")\n",
        "\n",
        "    # 1-2. get object path without the bucketname\n",
        "    objectpath = \"/\".join(gcs_source_uri[1:])\n",
        "\n",
        "    # 1-3. read the object to get value set by OutputArtifact from FileListGen\n",
        "    blob = bucket.blob(objectpath)\n",
        "    logging.info(f\"objectpath: {objectpath}\")\n",
        "\n",
        "    gcs_source = f\"gs://{blob.download_as_text()}\"\n",
        "\n",
        "    # Get Model\n",
        "    vertex_ai.init(project=project, location=location)\n",
        "\n",
        "    model = vertex_ai.Model.list(\n",
        "        filter=f\"display_name={model_resource_name}\", order_by=\"update_time\"\n",
        "    )[-1]\n",
        "\n",
        "    # Batch Predictions\n",
        "    logging.info(\"Starting batch prediction job.\")\n",
        "    logging.info(f\"GCS path where file list is: {gcs_source}\")\n",
        "\n",
        "    batch_prediction_job = model.batch_predict(\n",
        "        job_display_name=job_display_name,\n",
        "        instances_format=instances_format,\n",
        "        gcs_source=gcs_source,\n",
        "        gcs_destination_prefix=gcs_destination,\n",
        "        machine_type=machine_type,\n",
        "        accelerator_count=accelerator_count,\n",
        "        accelerator_type=accelerator_type,\n",
        "        starting_replica_count=starting_replica_count,\n",
        "        max_replica_count=max_replica_count,\n",
        "        sync=True,\n",
        "    )\n",
        "\n",
        "    logging.info(batch_prediction_job.display_name)\n",
        "    logging.info(batch_prediction_job.resource_name)\n",
        "    logging.info(batch_prediction_job.state)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing batch_prediction_vertex.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "bjz3oQvTLxPG"
      },
      "source": [
        "### **PerformanceEvaluator Component**\n",
        "- Calculate any performance metrics \n",
        "- Outputs if the model performance is above or below the given threshold\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - predictions\n",
        "  - threshold\n",
        "- output\n",
        "  - `True` or `False` by the threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mc4Sm0gF65F"
      },
      "source": [
        "_evaluator_module_file = 'batch_pred_evaluator.py'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv-UAqCHFvU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3608066-b6cb-435c-97bb-aaabbb89a11e"
      },
      "source": [
        "%%writefile {_evaluator_module_file}\n",
        "\n",
        "# Reference: https://bit.ly/vertex-batch\n",
        "\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "\n",
        "from absl import logging\n",
        "import os\n",
        "import json\n",
        "\n",
        "@component\n",
        "def PerformanceEvaluator(\n",
        "    gcs_destination: Parameter[str],\n",
        "    local_directory: Parameter[str],\n",
        "    threshold: Parameter[float],\n",
        "    trigger_pipeline: OutputArtifact[Dataset],\n",
        "):\n",
        "    full_gcs_results_dir = f\"{gcs_destination}/{local_directory}\"\n",
        "\n",
        "    # Create missing directories.\n",
        "    os.makedirs(local_directory, exist_ok=True)\n",
        "\n",
        "    # Get the Cloud Storage paths for each result.\n",
        "    os.system(f\"gsutil -m cp -r {full_gcs_results_dir} {local_directory}\")\n",
        "\n",
        "    # Get most recently modified directory.\n",
        "    latest_directory = max(\n",
        "        [os.path.join(local_directory, d) for d in os.listdir(local_directory)],\n",
        "        key=os.path.getmtime,\n",
        "    )\n",
        "\n",
        "    # Get downloaded results in directory.\n",
        "    results_files = []\n",
        "    for dirpath, subdirs, files in os.walk(latest_directory):\n",
        "        for file in files:\n",
        "            if file.startswith(\"prediction.results\"):\n",
        "                results_files.append(os.path.join(dirpath, file))\n",
        "\n",
        "    # Consolidate all the results into a list.\n",
        "    results = []\n",
        "    for results_file in results_files:\n",
        "        # Download each result.\n",
        "        with open(results_file, \"r\") as file:\n",
        "            results.extend([json.loads(line) for line in file.readlines()])\n",
        "\n",
        "    # Calculate performance.\n",
        "    num_correct = 0\n",
        "\n",
        "    for result in results:\n",
        "        label = os.path.basename(result[\"instance\"]).split(\"_\")[0]\n",
        "        prediction = result[\"prediction\"][\"label\"]\n",
        "\n",
        "        if label == prediction:\n",
        "            num_correct = num_correct + 1\n",
        "\n",
        "    accuracy = num_correct / len(results)\n",
        "    logging.info(f\"Accuracy: {accuracy*100}%\")\n",
        "    trigger_pipeline.set_string_custom_property(\"result\", str(accuracy >= threshold))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing batch_pred_evaluator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr98263MyoVn"
      },
      "source": [
        "### SpanPreparator\n",
        "- Compress the list of raw data into `tfrecord` and upload it to designated GCS location where `ImportExampleGen` in training pipeline wil consume."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwGwD1PyyrVd"
      },
      "source": [
        "_span_preparator_module_file = 'span_preparator.py'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1VfcBIJyvKs",
        "outputId": "f45c7aa3-fa45-4e67-f343-cccfad4d3817"
      },
      "source": [
        "%%writefile {_span_preparator_module_file}\n",
        "\n",
        "import tfx\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.dsl.component.experimental.annotations import OutputArtifact, InputArtifact\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "from google.cloud import storage\n",
        "from absl import logging\n",
        "\n",
        "from datetime import datetime\n",
        "import tensorflow as tf \n",
        "import random\n",
        "import gzip\n",
        "import os\n",
        "\n",
        "# Label-mapping.\n",
        "LABEL_DICT = {\n",
        "    \"airplane\": 0,\n",
        "    \"automobile\": 1,\n",
        "    \"bird\": 2,\n",
        "    \"cat\": 3,\n",
        "    \"deer\": 4,\n",
        "    \"dog\": 5,\n",
        "    \"frog\": 6,\n",
        "    \"horse\": 7,\n",
        "    \"ship\": 8,\n",
        "    \"truck\": 9,\n",
        "}\n",
        "\n",
        "# Images are byte-strings.\n",
        "def _bytestring_feature(list_of_bytestrings):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n",
        "\n",
        "# Classes would be integers.\n",
        "def _int_feature(list_of_ints): \n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n",
        "\n",
        "# Function that prepares a record for the tfrecord file\n",
        "# a record contains the image and its label.\n",
        "def to_tfrecord(img_bytes, label):  \n",
        "    feature = {\n",
        "      \"image\": _bytestring_feature([img_bytes]), \n",
        "      \"label\": _int_feature([label]),             \n",
        "    }\n",
        "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "def write_tfrecords(filepaths, dest_gcs, tfrecord_filename, new_span, is_train):\n",
        "    # For this project, we are serializing the images in one TFRecord only.\n",
        "    # For more realistic purposes, this should be sharded. \n",
        "    folder = 'train' if is_train else 'test'\n",
        "\n",
        "    with tf.io.TFRecordWriter(tfrecord_filename) as writer: \n",
        "        for path in filepaths:\n",
        "            image_string = tf.io.read_file(path).numpy()\n",
        "            class_name = path.split(\"/\")[-1].split(\"_\")[0]\n",
        "            label = LABEL_DICT[class_name]\n",
        "\n",
        "            example = to_tfrecord(image_string, label)\n",
        "            writer.write(example.SerializeToString())\n",
        "\n",
        "    # Copy over the zipped TFRecord file to the GCS Bucket and \n",
        "    # remove the temporary files.\n",
        "    logging.info(f\"gsutil cp {tfrecord_filename} {dest_gcs}/span-{new_span}/{folder}/\")\n",
        "    os.system(f\"gsutil cp {tfrecord_filename} {dest_gcs}/span-{new_span}/{folder}/\")\n",
        "    os.remove(tfrecord_filename)\n",
        "\n",
        "@component\n",
        "def SpanPreparator(\n",
        "    is_retrain: InputArtifact[Dataset],\n",
        "    gcs_source_bucket: Parameter[str],\n",
        "    gcs_destination_bucket: Parameter[str],\n",
        "):\n",
        "  if is_retrain.get_string_custom_property('result') == 'False':\n",
        "    last_span_str = tf.io.gfile.glob(f\"{gcs_destination_bucket}/span-*\")[-1]\n",
        "    last_span = int(last_span_str.split('-')[-1])\n",
        "    new_span = last_span + 1\n",
        "\n",
        "    timestamp = datetime.utcnow().strftime(\"%y%m%d-%H%M%S\")\n",
        "    \n",
        "    image_paths = tf.io.gfile.glob(f\"gs://{gcs_source_bucket}/*.jpg\")\n",
        "    logging.info(image_paths)\n",
        "    random.shuffle(image_paths)  \n",
        "    \n",
        "    val_split = 0.2\n",
        "\n",
        "    split_index = int(len(image_paths) * (1 - val_split))\n",
        "    training_paths = image_paths[:split_index]\n",
        "    validation_paths = image_paths[split_index:]\n",
        "\n",
        "    write_tfrecords(training_paths, gcs_destination_bucket, tfrecord_filename=f\"new_training_data_{timestamp}.tfrecord\", new_span=new_span, is_train=True)\n",
        "    write_tfrecords(validation_paths, gcs_destination_bucket, tfrecord_filename=f\"new_validation_data_{timestamp}.tfrecord\", new_span=new_span, is_train=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting span_preparator.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "N79u9i-tL5Jm"
      },
      "source": [
        "### PipelineTrigger Component\n",
        "- Trigger the training pipeline based on the `True` or `False` value by the threshold\n",
        "\n",
        "**Spec**\n",
        "- input\n",
        "  - `True` or `False` by threshold\n",
        "  - pipeline name to be triggered\n",
        "  - GCS path where the pipeline spec is \n",
        "  - GCP project ID\n",
        "  - GCP region\n",
        "- output\n",
        "  - None"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWxscRQ03h9c"
      },
      "source": [
        "_pipeline_trigger_module_file = 'training_pipeline_trigger.py'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuuW5kfa3hfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6f87e9-415b-4ca1-c6aa-c804554a2ddb"
      },
      "source": [
        "%%writefile {_pipeline_trigger_module_file}\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "from kfp.v2.google.client import AIPlatformClient\n",
        "from tfx.dsl.component.experimental.annotations import Parameter, InputArtifact\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.types.experimental.simple_artifacts import Dataset\n",
        "\n",
        "from absl import logging\n",
        "\n",
        "@component\n",
        "def PipelineTrigger(\n",
        "\tis_retrain: InputArtifact[Dataset],\n",
        "\tpipeline_spec_path: Parameter[str],\n",
        "\tproject_id: Parameter[str],\n",
        "\tregion: Parameter[str],\n",
        "):\n",
        "\tif is_retrain.get_string_custom_property('result') == 'False':\n",
        "\t\t# Check if the pipeline spec exists.\n",
        "\t\tstorage_client = storage.Client()\n",
        "\n",
        "\t\tpath_parts = pipeline_spec_path.replace(\"gs://\", \"\").split(\"/\")\n",
        "\t\tbucket_name = path_parts[0]\n",
        "\t\tblob_name = \"/\".join(path_parts[1:])\n",
        "\n",
        "\t\tbucket = storage_client.bucket(bucket_name)\n",
        "\t\tblob = storage.Blob(bucket=bucket, name=blob_name)\n",
        "\n",
        "\t\tif not blob.exists(storage_client):\n",
        "\t\t\traise ValueError(f\"{pipeline_spec_path} does not exist.\")\n",
        "\n",
        "\t\t# Initialize Vertex AI API client and submit for pipeline execution.\n",
        "\t\tapi_client = AIPlatformClient(project_id=project_id, region=region)\n",
        "\n",
        "\t\tresponse = api_client.create_run_from_job_spec(\n",
        "\t\t\tjob_spec_path=pipeline_spec_path,\n",
        "\t\t\tenable_caching=True,\n",
        "\t\t)\n",
        "\n",
        "\t\tlogging.info(response)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting training_pipeline_trigger.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQNNM_Y5DNI"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjpDLeePZQR4"
      },
      "source": [
        "### Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnsPuwUNL4R4"
      },
      "source": [
        "# This bucket will be responsible for storing the pipeline related artifacts.\n",
        "GOOGLE_CLOUD_PROJECT = 'gcp-ml-172005'    #@param {type:\"string\"}\n",
        "GOOGLE_CLOUD_REGION = 'us-central1'      \n",
        "\n",
        "GCS_BUCKET_NAME = 'cifar10-experimental-csp2'    #@param {type:\"string\"}\n",
        "\n",
        "MODEL_RESOURCE_NAME = 'resnet_cifar_latest' #@param {type: \"string\"}\n",
        "\n",
        "TEST_FILENAME = 'test-images.txt' #@param {type:\"string\"}\n",
        "TEST_GCS_BUCKET = 'batch-prediction-collection-3' #@param {type:\"string\"}\n",
        "TEST_GCS_PREFIX = '' #@param {type: \"string\"}\n",
        "\n",
        "TRAINING_PIPELINE_SPEC = 'gs://cifar10-experimental-csp2/pipeline_root/continuous-adaptation-for-data-changes/continuous-adaptation-for-data-changes_pipeline.json' #@param {type: \"string\"}\n",
        "TRAINING_DATA_PATH = 'gs://cifar10-csp-public2' #@param {type: \"string\"}\n",
        "\n",
        "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
        "    from absl import logging\n",
        "    logging.error('Please set all required parameters.')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOljVxpi5Xo1",
        "outputId": "f31941d2-b606-4fd7-f8ba-e574fc816792"
      },
      "source": [
        "PIPELINE_NAME = 'continuous-adaptation-for-data-changes-batch'\n",
        "\n",
        "# Path to various pipeline artifact.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIPELINE_ROOT: gs://cifar10-experimental-csp2/pipeline_root/continuous-adaptation-for-data-changes-batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzy9y2EGZQR5"
      },
      "source": [
        "### Custom Docker image to run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eic7WtdY5iTz"
      },
      "source": [
        "!mkdir -p ./custom_components\n",
        "!touch ./custom_components/__init__.py\n",
        "!cp -r {_file_list_gen_module_file} {_batch_pred_module_file} {_evaluator_module_file} {_span_preparator_module_file} {_pipeline_trigger_module_file} custom_components"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rllJsXU9JwUZ",
        "outputId": "a03f2f4c-f7d3-4f42-d25a-a8367b9e8847"
      },
      "source": [
        "!ls -lh custom_components"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24K\n",
            "-rw-r--r-- 1 root root 2.0K Oct 12 00:25 batch_pred_evaluator.py\n",
            "-rw-r--r-- 1 root root 2.5K Oct 12 00:25 batch_prediction_vertex.py\n",
            "-rw-r--r-- 1 root root 1.6K Oct 12 00:25 file_list_gen.py\n",
            "-rw-r--r-- 1 root root    0 Oct 12 00:25 __init__.py\n",
            "drwxr-xr-x 2 root root 4.0K Oct 12 00:25 __pycache__\n",
            "-rw-r--r-- 1 root root 3.2K Oct 12 00:25 span_preparator.py\n",
            "-rw-r--r-- 1 root root 1.3K Oct 12 00:25 training_pipeline_trigger.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf-5AjJQ5w9Z",
        "outputId": "1fb3f223-ab9b-44de-d691-be55f70117e8"
      },
      "source": [
        "DISPLAY_NAME = \"batch-predictions-pipeline\"\n",
        "VERSION = \"tfx-1-2-0-26\"\n",
        "TFX_IMAGE_URI = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{DISPLAY_NAME}:{VERSION}\"\n",
        "print(f\"URI of the custom image: {TFX_IMAGE_URI}\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URI of the custom image: gcr.io/gcp-ml-172005/batch-predictions-pipeline:tfx-1-2-0-26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvv4-FIG58P9",
        "outputId": "8a0a59bd-119a-437f-e097-df38e3c7e28e"
      },
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
        "RUN mkdir -p custom_components\n",
        "COPY custom_components/* ./custom_components/\n",
        "RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage kfp==1.6.1"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I86iu9-76DQi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b3d2286-60ba-44d0-8709-3a3f4241371f"
      },
      "source": [
        "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary tarball archive of 48 file(s) totalling 54.5 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://gcp-ml-172005_cloudbuild/source/1633998373.120888-800a41ebee24435a813d5487de391a29.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/gcp-ml-172005/locations/global/builds/18b252e7-d4a7-400d-a5ff-20f5ba0eb8e8].\n",
            "Logs are available at [https://console.cloud.google.com/cloud-build/builds/18b252e7-d4a7-400d-a5ff-20f5ba0eb8e8?project=874401645461].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"18b252e7-d4a7-400d-a5ff-20f5ba0eb8e8\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://gcp-ml-172005_cloudbuild/source/1633998373.120888-800a41ebee24435a813d5487de391a29.tgz#1633998383239942\n",
            "Copying gs://gcp-ml-172005_cloudbuild/source/1633998373.120888-800a41ebee24435a813d5487de391a29.tgz#1633998383239942...\n",
            "/ [1 files][  6.5 MiB/  6.5 MiB]                                                \n",
            "Operation completed over 1 objects/6.5 MiB.\n",
            "tar: .config/gce: time stamp 2040-01-01 00:00:00 is 574990346.910051548 s in the future\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon   57.2MB\n",
            "Step 1/4 : FROM gcr.io/tfx-oss-public/tfx:1.2.0\n",
            "1.2.0: Pulling from tfx-oss-public/tfx\n",
            "25fa05cd42bd: Pulling fs layer\n",
            "2d6e353a95ec: Pulling fs layer\n",
            "14d7996407de: Pulling fs layer\n",
            "0c9c6fc70f16: Pulling fs layer\n",
            "c3c76be11512: Pulling fs layer\n",
            "ab6e5a9c78ee: Pulling fs layer\n",
            "7bc1690abd59: Pulling fs layer\n",
            "f5b4dd7682bc: Pulling fs layer\n",
            "d6897660f71d: Pulling fs layer\n",
            "174d792fb622: Pulling fs layer\n",
            "5f8143275aca: Pulling fs layer\n",
            "56646f115483: Pulling fs layer\n",
            "7bc1690abd59: Waiting\n",
            "0c9c6fc70f16: Waiting\n",
            "ab6e5a9c78ee: Waiting\n",
            "c3c76be11512: Waiting\n",
            "5f8143275aca: Waiting\n",
            "798922b52524: Pulling fs layer\n",
            "e2699a9f592b: Pulling fs layer\n",
            "f43e7d1c07e4: Pulling fs layer\n",
            "1e71d5e9923d: Pulling fs layer\n",
            "bf6ae2a2e250: Pulling fs layer\n",
            "e49679b748d5: Pulling fs layer\n",
            "80208bd6f7fb: Pulling fs layer\n",
            "b83c16bef138: Pulling fs layer\n",
            "9d1427033824: Pulling fs layer\n",
            "c0028679f003: Pulling fs layer\n",
            "09c222e7ff04: Pulling fs layer\n",
            "ae6048a3aec1: Pulling fs layer\n",
            "d6897660f71d: Waiting\n",
            "1ced637de50b: Pulling fs layer\n",
            "174d792fb622: Waiting\n",
            "762ff1eb7f16: Pulling fs layer\n",
            "f6f8f4265c8c: Pulling fs layer\n",
            "595b1c49222a: Pulling fs layer\n",
            "b6c7eb38f366: Pulling fs layer\n",
            "520be5017b4d: Pulling fs layer\n",
            "0a1aacb7e387: Pulling fs layer\n",
            "798922b52524: Waiting\n",
            "8f605134caf9: Pulling fs layer\n",
            "189ba322d030: Pulling fs layer\n",
            "af92447b9198: Pulling fs layer\n",
            "b83c16bef138: Waiting\n",
            "e2699a9f592b: Waiting\n",
            "9d1427033824: Waiting\n",
            "1ced637de50b: Waiting\n",
            "762ff1eb7f16: Waiting\n",
            "c0028679f003: Waiting\n",
            "f43e7d1c07e4: Waiting\n",
            "f6f8f4265c8c: Waiting\n",
            "09c222e7ff04: Waiting\n",
            "1e71d5e9923d: Waiting\n",
            "595b1c49222a: Waiting\n",
            "ae6048a3aec1: Waiting\n",
            "bf6ae2a2e250: Waiting\n",
            "b6c7eb38f366: Waiting\n",
            "520be5017b4d: Waiting\n",
            "189ba322d030: Waiting\n",
            "e49679b748d5: Waiting\n",
            "af92447b9198: Waiting\n",
            "8f605134caf9: Waiting\n",
            "f5b4dd7682bc: Waiting\n",
            "56646f115483: Waiting\n",
            "0a1aacb7e387: Waiting\n",
            "14d7996407de: Verifying Checksum\n",
            "14d7996407de: Download complete\n",
            "0c9c6fc70f16: Verifying Checksum\n",
            "0c9c6fc70f16: Download complete\n",
            "25fa05cd42bd: Verifying Checksum\n",
            "25fa05cd42bd: Download complete\n",
            "2d6e353a95ec: Verifying Checksum\n",
            "2d6e353a95ec: Download complete\n",
            "c3c76be11512: Download complete\n",
            "7bc1690abd59: Download complete\n",
            "d6897660f71d: Verifying Checksum\n",
            "d6897660f71d: Download complete\n",
            "25fa05cd42bd: Pull complete\n",
            "2d6e353a95ec: Pull complete\n",
            "14d7996407de: Pull complete\n",
            "0c9c6fc70f16: Pull complete\n",
            "c3c76be11512: Pull complete\n",
            "f5b4dd7682bc: Verifying Checksum\n",
            "f5b4dd7682bc: Download complete\n",
            "5f8143275aca: Verifying Checksum\n",
            "5f8143275aca: Download complete\n",
            "ab6e5a9c78ee: Verifying Checksum\n",
            "ab6e5a9c78ee: Download complete\n",
            "174d792fb622: Download complete\n",
            "e2699a9f592b: Download complete\n",
            "f43e7d1c07e4: Verifying Checksum\n",
            "f43e7d1c07e4: Download complete\n",
            "798922b52524: Verifying Checksum\n",
            "798922b52524: Download complete\n",
            "bf6ae2a2e250: Verifying Checksum\n",
            "bf6ae2a2e250: Download complete\n",
            "e49679b748d5: Verifying Checksum\n",
            "e49679b748d5: Download complete\n",
            "80208bd6f7fb: Verifying Checksum\n",
            "80208bd6f7fb: Download complete\n",
            "b83c16bef138: Verifying Checksum\n",
            "b83c16bef138: Download complete\n",
            "9d1427033824: Verifying Checksum\n",
            "9d1427033824: Download complete\n",
            "c0028679f003: Verifying Checksum\n",
            "c0028679f003: Download complete\n",
            "09c222e7ff04: Download complete\n",
            "ae6048a3aec1: Download complete\n",
            "56646f115483: Verifying Checksum\n",
            "56646f115483: Download complete\n",
            "1e71d5e9923d: Verifying Checksum\n",
            "1e71d5e9923d: Download complete\n",
            "f6f8f4265c8c: Verifying Checksum\n",
            "f6f8f4265c8c: Download complete\n",
            "1ced637de50b: Download complete\n",
            "b6c7eb38f366: Verifying Checksum\n",
            "b6c7eb38f366: Download complete\n",
            "520be5017b4d: Verifying Checksum\n",
            "520be5017b4d: Download complete\n",
            "0a1aacb7e387: Verifying Checksum\n",
            "0a1aacb7e387: Download complete\n",
            "8f605134caf9: Verifying Checksum\n",
            "8f605134caf9: Download complete\n",
            "189ba322d030: Verifying Checksum\n",
            "189ba322d030: Download complete\n",
            "af92447b9198: Verifying Checksum\n",
            "af92447b9198: Download complete\n",
            "595b1c49222a: Verifying Checksum\n",
            "595b1c49222a: Download complete\n",
            "762ff1eb7f16: Verifying Checksum\n",
            "762ff1eb7f16: Download complete\n",
            "ab6e5a9c78ee: Pull complete\n",
            "7bc1690abd59: Pull complete\n",
            "f5b4dd7682bc: Pull complete\n",
            "d6897660f71d: Pull complete\n",
            "174d792fb622: Pull complete\n",
            "5f8143275aca: Pull complete\n",
            "56646f115483: Pull complete\n",
            "798922b52524: Pull complete\n",
            "e2699a9f592b: Pull complete\n",
            "f43e7d1c07e4: Pull complete\n",
            "1e71d5e9923d: Pull complete\n",
            "bf6ae2a2e250: Pull complete\n",
            "e49679b748d5: Pull complete\n",
            "80208bd6f7fb: Pull complete\n",
            "b83c16bef138: Pull complete\n",
            "9d1427033824: Pull complete\n",
            "c0028679f003: Pull complete\n",
            "09c222e7ff04: Pull complete\n",
            "ae6048a3aec1: Pull complete\n",
            "1ced637de50b: Pull complete\n",
            "762ff1eb7f16: Pull complete\n",
            "f6f8f4265c8c: Pull complete\n",
            "595b1c49222a: Pull complete\n",
            "b6c7eb38f366: Pull complete\n",
            "520be5017b4d: Pull complete\n",
            "0a1aacb7e387: Pull complete\n",
            "8f605134caf9: Pull complete\n",
            "189ba322d030: Pull complete\n",
            "af92447b9198: Pull complete\n",
            "Digest: sha256:eba9e7b7d9131eb5b05434feaafc4676268ad805e4b97218f58994ad2714be67\n",
            "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.2.0\n",
            " ---> 0e86fadcc60c\n",
            "Step 2/4 : RUN mkdir -p custom_components\n",
            " ---> Running in c5a026642bac\n",
            "Removing intermediate container c5a026642bac\n",
            " ---> 1b8869a017e8\n",
            "Step 3/4 : COPY custom_components/* ./custom_components/\n",
            " ---> 0c3b850257a1\n",
            "Step 4/4 : RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage kfp==1.6.1\n",
            " ---> Running in a22e416625c3\n",
            "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (0.7.1)\n",
            "Collecting google-cloud-aiplatform\n",
            "  Downloading google_cloud_aiplatform-1.5.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (1.41.1)\n",
            "Collecting google-cloud-storage\n",
            "  Downloading google_cloud_storage-1.42.3-py2.py3-none-any.whl (105 kB)\n",
            "Collecting kfp==1.6.1\n",
            "  Downloading kfp-1.6.1.tar.gz (221 kB)\n",
            "Collecting absl-py<=0.11,>=0.9\n",
            "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: PyYAML<6,>=5.3 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (5.4.1)\n",
            "Requirement already satisfied: kubernetes<13,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (12.0.1)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (1.12.8)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (1.34.0)\n",
            "Collecting requests-toolbelt<1,>=0.8.0\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "Requirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (1.6.0)\n",
            "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
            "  Downloading kfp-server-api-1.7.0.tar.gz (52 kB)\n",
            "Requirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (3.2.0)\n",
            "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (0.8.9)\n",
            "Requirement already satisfied: click<8,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (7.1.2)\n",
            "Collecting Deprecated<2,>=1.2.7\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting strip-hints<1,>=0.1.8\n",
            "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
            "Collecting docstring-parser<1,>=0.7.3\n",
            "  Downloading docstring_parser-0.11.tar.gz (22 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'\n",
            "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (0.1.9)\n",
            "Collecting fire<1,>=0.3.1\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==1.6.1) (3.16.0)\n",
            "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.20.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (20.9)\n",
            "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.31.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.7.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.3.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.25.1)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.15.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp==1.6.1) (1.12.1)\n",
            "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp==1.6.1) (1.1.0)\n",
            "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2021.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (49.6.0.post20210108)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.34.1)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.1) (0.1.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp==1.6.1) (0.19.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.1) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.1) (0.2.7)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp==1.6.1) (4.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (2.20)\n",
            "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp==1.6.1) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.1) (4.6.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.1) (20.3.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp==1.6.1) (0.17.3)\n",
            "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.1) (1.26.6)\n",
            "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.1) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp==1.6.1) (2.8.2)\n",
            "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.1) (1.3.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<13,>=8.0.0->kfp==1.6.1) (0.57.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp==1.6.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
            "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp==1.6.1) (0.36.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp==1.6.1) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<13,>=8.0.0->kfp==1.6.1) (3.1.1)\n",
            "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
            "  Building wheel for kfp (setup.py): started\n",
            "  Building wheel for kfp (setup.py): finished with status 'done'\n",
            "  Created wheel for kfp: filename=kfp-1.6.1-py3-none-any.whl size=301570 sha256=9ad267f485188414f586c7c70e83ea54522897996cac3c99ed1a98bbe9b87809\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/e9/fe/2eb7a670d99e0b638de9882d87695010501512b61ce546d675\n",
            "  Building wheel for docstring-parser (PEP 517): started\n",
            "  Building wheel for docstring-parser (PEP 517): finished with status 'done'\n",
            "  Created wheel for docstring-parser: filename=docstring_parser-0.11-py3-none-any.whl size=31531 sha256=14603c2d2a51bb4210a99ca8a58086468725f2d11e47788b5b4c583765122352\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/ba/2a/1376f9ea0b3f20a9700b4f1b4ee3cefe69b4a96e26a28e0240\n",
            "  Building wheel for fire (setup.py): started\n",
            "  Building wheel for fire (setup.py): finished with status 'done'\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=d0255d2c114f1511c8fd95ed0d177bb943a42ddbe5bf0755cf4cd7dd0990ec1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "  Building wheel for kfp-server-api (setup.py): started\n",
            "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
            "  Created wheel for kfp-server-api: filename=kfp_server_api-1.7.0-py3-none-any.whl size=92619 sha256=d7d446bb2b6244403ef999688b8ae05488dd68458a8e506c10c3902fdae49815\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/f0/36/cd1c7475b12b2541f90e4ab9413e59756a11262c1307a97633\n",
            "  Building wheel for strip-hints (setup.py): started\n",
            "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
            "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=f8b3720e18d01118c9a974455e6c528eb9136e8a4acd6f5a5a6843646d42744e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
            "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
            "Installing collected packages: strip-hints, requests-toolbelt, kfp-server-api, google-cloud-storage, fire, docstring-parser, Deprecated, absl-py, kfp, google-cloud-aiplatform\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.41.1\n",
            "    Uninstalling google-cloud-storage-1.41.1:\n",
            "      Successfully uninstalled google-cloud-storage-1.41.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Attempting uninstall: google-cloud-aiplatform\n",
            "    Found existing installation: google-cloud-aiplatform 0.7.1\n",
            "    Uninstalling google-cloud-aiplatform-0.7.1:\n",
            "      Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
            "Successfully installed Deprecated-1.2.13 absl-py-0.11.0 docstring-parser-0.11 fire-0.4.0 google-cloud-aiplatform-1.5.0 google-cloud-storage-1.42.3 kfp-1.6.1 kfp-server-api-1.7.0 requests-toolbelt-0.9.1 strip-hints-0.1.10\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
            "tfx 1.2.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.5.0 which is incompatible.\n",
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "WARNING: You are using pip version 21.2.4; however, version 21.3 is available.\n",
            "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\n",
            "Removing intermediate container a22e416625c3\n",
            " ---> 1aaeba5e3a39\n",
            "Successfully built 1aaeba5e3a39\n",
            "Successfully tagged gcr.io/gcp-ml-172005/batch-predictions-pipeline:tfx-1-2-0-26\n",
            "PUSH\n",
            "Pushing gcr.io/gcp-ml-172005/batch-predictions-pipeline:tfx-1-2-0-26\n",
            "The push refers to repository [gcr.io/gcp-ml-172005/batch-predictions-pipeline]\n",
            "597dad583e5d: Preparing\n",
            "51181a51a4e6: Preparing\n",
            "504bb2ba58f4: Preparing\n",
            "d42c05f73feb: Preparing\n",
            "3119d30f29a9: Preparing\n",
            "121c9dfc7bce: Preparing\n",
            "868786b3710b: Preparing\n",
            "5bb1aa5df10d: Preparing\n",
            "f028010939aa: Preparing\n",
            "dc99c4ea3a81: Preparing\n",
            "37b508c5711b: Preparing\n",
            "756ab564e194: Preparing\n",
            "2ae86808a3d1: Preparing\n",
            "1dccbdf9b557: Preparing\n",
            "cfcbdbc2b748: Preparing\n",
            "937ab8f29c2e: Preparing\n",
            "5d417b2f7486: Preparing\n",
            "d6a297a3e6e4: Preparing\n",
            "6474a5e8117f: Preparing\n",
            "fe498124ed57: Preparing\n",
            "d5454704bb3d: Preparing\n",
            "fb896ef24b4b: Preparing\n",
            "dc99c4ea3a81: Waiting\n",
            "37b508c5711b: Waiting\n",
            "756ab564e194: Waiting\n",
            "2ae86808a3d1: Waiting\n",
            "1dccbdf9b557: Waiting\n",
            "cfcbdbc2b748: Waiting\n",
            "937ab8f29c2e: Waiting\n",
            "5d417b2f7486: Waiting\n",
            "d6a297a3e6e4: Waiting\n",
            "6474a5e8117f: Waiting\n",
            "fe498124ed57: Waiting\n",
            "121c9dfc7bce: Waiting\n",
            "868786b3710b: Waiting\n",
            "5bb1aa5df10d: Waiting\n",
            "f028010939aa: Waiting\n",
            "d5454704bb3d: Waiting\n",
            "5087113f67c8: Preparing\n",
            "2a92857a1d48: Preparing\n",
            "0ded97864c52: Preparing\n",
            "5087113f67c8: Waiting\n",
            "fb896ef24b4b: Waiting\n",
            "b50bbaac3e32: Preparing\n",
            "262ea1af4c10: Preparing\n",
            "2a92857a1d48: Waiting\n",
            "0ded97864c52: Waiting\n",
            "b420a468ca49: Preparing\n",
            "608c205798d1: Preparing\n",
            "b420a468ca49: Waiting\n",
            "0760cd6d4269: Preparing\n",
            "fb4755c89c2a: Preparing\n",
            "22cfb9034da6: Preparing\n",
            "608c205798d1: Waiting\n",
            "8bec4fbfce85: Preparing\n",
            "3b129ca3db46: Preparing\n",
            "fb4755c89c2a: Waiting\n",
            "64cb1a1930ab: Preparing\n",
            "3b129ca3db46: Waiting\n",
            "8bec4fbfce85: Waiting\n",
            "600ef5a43f1f: Preparing\n",
            "8f8f0266f834: Preparing\n",
            "64cb1a1930ab: Waiting\n",
            "600ef5a43f1f: Waiting\n",
            "8f8f0266f834: Waiting\n",
            "d42c05f73feb: Layer already exists\n",
            "3119d30f29a9: Layer already exists\n",
            "121c9dfc7bce: Layer already exists\n",
            "868786b3710b: Layer already exists\n",
            "5bb1aa5df10d: Layer already exists\n",
            "f028010939aa: Layer already exists\n",
            "dc99c4ea3a81: Layer already exists\n",
            "37b508c5711b: Layer already exists\n",
            "756ab564e194: Layer already exists\n",
            "2ae86808a3d1: Layer already exists\n",
            "1dccbdf9b557: Layer already exists\n",
            "cfcbdbc2b748: Layer already exists\n",
            "937ab8f29c2e: Layer already exists\n",
            "5d417b2f7486: Layer already exists\n",
            "d6a297a3e6e4: Layer already exists\n",
            "6474a5e8117f: Layer already exists\n",
            "fe498124ed57: Layer already exists\n",
            "d5454704bb3d: Layer already exists\n",
            "fb896ef24b4b: Layer already exists\n",
            "5087113f67c8: Layer already exists\n",
            "2a92857a1d48: Layer already exists\n",
            "0ded97864c52: Layer already exists\n",
            "b50bbaac3e32: Layer already exists\n",
            "262ea1af4c10: Layer already exists\n",
            "b420a468ca49: Layer already exists\n",
            "608c205798d1: Layer already exists\n",
            "0760cd6d4269: Layer already exists\n",
            "fb4755c89c2a: Layer already exists\n",
            "22cfb9034da6: Layer already exists\n",
            "8bec4fbfce85: Layer already exists\n",
            "3b129ca3db46: Layer already exists\n",
            "64cb1a1930ab: Layer already exists\n",
            "600ef5a43f1f: Layer already exists\n",
            "8f8f0266f834: Layer already exists\n",
            "504bb2ba58f4: Pushed\n",
            "51181a51a4e6: Pushed\n",
            "597dad583e5d: Pushed\n",
            "tfx-1-2-0-26: digest: sha256:c20c7d1216de6ede0fb277de0ead4934598eacb5b86dc6b22062c556a1245820 size: 8100\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                        STATUS\n",
            "18b252e7-d4a7-400d-a5ff-20f5ba0eb8e8  2021-10-12T00:26:23+00:00  4M22S     gs://gcp-ml-172005_cloudbuild/source/1633998373.120888-800a41ebee24435a813d5487de391a29.tgz  gcr.io/gcp-ml-172005/batch-predictions-pipeline:tfx-1-2-0-26  SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMl_qx32ZQR6"
      },
      "source": [
        "### Create the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1OrDsPgKrfS"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDNAaIjB6HWy"
      },
      "source": [
        "import tfx\n",
        "from tfx.orchestration.pipeline import Pipeline\n",
        "from custom_components.file_list_gen import FileListGen\n",
        "from custom_components.batch_prediction_vertex import BatchPredictionGen\n",
        "from custom_components.batch_pred_evaluator import PerformanceEvaluator\n",
        "from custom_components.training_pipeline_trigger import PipelineTrigger\n",
        "from custom_components.span_preparator import SpanPreparator\n",
        "\n",
        "def _create_pipeline(\n",
        "    pipeline_name: str,\n",
        "    pipeline_root: str,\n",
        "    data_gcs_bucket: str,\n",
        "    data_gcs_prefix: str,\n",
        "    batch_job_gcs: str,\n",
        "    job_display_name: str,\n",
        "    model_resource_name: str,\n",
        "    project_id: str,\n",
        "    region: str,\n",
        "    threshold: float,\n",
        "    data_gcs_destination: str,\n",
        "    training_pipeline_spec: str,\n",
        ") -> Pipeline:\n",
        "\n",
        "    # Generate a file list for batch preditions. \n",
        "    # More details on the structure of this file here:\n",
        "    # https://bit.ly/3BzfHVu.\n",
        "    filelist_gen = FileListGen(\n",
        "        project=project_id,\n",
        "        gcs_source_bucket=data_gcs_bucket,\n",
        "        gcs_source_prefix=data_gcs_prefix,\n",
        "    ).with_id(\"filelist_gen\")\n",
        "\n",
        "    # Submit a batch prediction job.\n",
        "    batch_pred_component = BatchPredictionGen(\n",
        "        project=project_id,\n",
        "        location=region,\n",
        "        job_display_name=job_display_name,\n",
        "        model_resource_name=model_resource_name,\n",
        "        gcs_source=filelist_gen.outputs[\"outpath\"],\n",
        "        gcs_destination=f\"gs://{batch_job_gcs}/results/\",\n",
        "        accelerator_count=0,\n",
        "        accelerator_type=None,\n",
        "    ).with_id(\"bulk_inferer_vertex\")\n",
        "    batch_pred_component.add_upstream_node(filelist_gen)\n",
        "\n",
        "    # Evaluate the performance of the predictions. \n",
        "    # In a real-world project, this evaluation takes place\n",
        "    # separately, typically with the help of domain experts. \n",
        "    final_gcs_destination=f\"gs://{batch_job_gcs}/results/\"\n",
        "    evaluator = PerformanceEvaluator(\n",
        "        gcs_destination=f'gs://{final_gcs_destination.split(\"/\")[2]}',\n",
        "        local_directory=final_gcs_destination.split(\"/\")[-2],\n",
        "        threshold=threshold\n",
        "    ).with_id(\"batch_prediction_evaluator\")\n",
        "    evaluator.add_upstream_node(batch_pred_component)\n",
        "\n",
        "    span_preparator = SpanPreparator(\n",
        "      is_retrain=evaluator.outputs[\"trigger_pipeline\"],\n",
        "      gcs_source_bucket=data_gcs_bucket,\n",
        "      gcs_destination_bucket=data_gcs_destination,\n",
        "    ).with_id(\"span_preparator\")\n",
        "    span_preparator.add_upstream_node(evaluator)\n",
        "\n",
        "    trigger = PipelineTrigger(\n",
        "      is_retrain=evaluator.outputs[\"trigger_pipeline\"],\n",
        "      pipeline_spec_path=training_pipeline_spec,\n",
        "      project_id=project_id,\n",
        "      region=region\n",
        "    ).with_id(\"training_pipeline_trigger\")\n",
        "    trigger.add_upstream_node(span_preparator)\n",
        "\n",
        "    components = [\n",
        "      filelist_gen, \n",
        "      batch_pred_component, \n",
        "      evaluator, \n",
        "      span_preparator,\n",
        "      trigger,\n",
        "    ]\n",
        "\n",
        "    return Pipeline(\n",
        "        pipeline_name=pipeline_name, pipeline_root=pipeline_root, components=components\n",
        "    )"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE4A6ApPZQR7"
      },
      "source": [
        "### Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi3BobZw7JiS"
      },
      "source": [
        "import os\n",
        "import tfx\n",
        "from tfx.orchestration.kubeflow.v2.kubeflow_v2_dag_runner import KubeflowV2DagRunner\n",
        "from tfx.orchestration.kubeflow.v2.kubeflow_v2_dag_runner import (\n",
        "    KubeflowV2DagRunnerConfig,\n",
        ")\n",
        "\n",
        "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + \"_pipeline.json\"\n",
        "THRESHOLD = 0.9\n",
        "\n",
        "# Important: We need to pass the custom Docker image URI to the\n",
        "# `KubeflowV2DagRunnerConfig` to take effect.\n",
        "runner = KubeflowV2DagRunner(\n",
        "    config=KubeflowV2DagRunnerConfig(default_image=TFX_IMAGE_URI),\n",
        "    output_filename=PIPELINE_DEFINITION_FILE,\n",
        ")\n",
        "\n",
        "_ = runner.run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        data_gcs_bucket=TEST_GCS_BUCKET,\n",
        "        data_gcs_prefix=TEST_GCS_PREFIX,\n",
        "        batch_job_gcs=GCS_BUCKET_NAME,\n",
        "        job_display_name=f\"{MODEL_RESOURCE_NAME}_{TIMESTAMP}\",\n",
        "        project_id=GOOGLE_CLOUD_PROJECT,\n",
        "        region=GOOGLE_CLOUD_REGION,\n",
        "        model_resource_name=MODEL_RESOURCE_NAME,\n",
        "        threshold=THRESHOLD,\n",
        "        data_gcs_destination=TRAINING_DATA_PATH,\n",
        "        training_pipeline_spec=TRAINING_PIPELINE_SPEC,\n",
        "    )\n",
        ")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "M6ofDcYP7ogx",
        "outputId": "7a92f4bc-abdf-4d77-9106-63e26d5163d5"
      },
      "source": [
        "from kfp.v2.google import client\n",
        "\n",
        "pipelines_client = client.AIPlatformClient(\n",
        "    project_id=GOOGLE_CLOUD_PROJECT,\n",
        "    region=GOOGLE_CLOUD_REGION,\n",
        ")\n",
        "\n",
        "_ = pipelines_client.create_run_from_job_spec(PIPELINE_DEFINITION_FILE, enable_caching=False)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/continuous-adaptation-for-data-changes-batch-20211012003221?project=gcp-ml-172005\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okNlX9toiu9A"
      },
      "source": [
        "This is how the pipeline should look like:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.ibb.co/nDGS5Xk/Screenshot-2021-09-27-at-9-46-41-AM.png\" width=500></ing>"
      ]
    }
  ]
}