{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "04_Cloud_Scheduler_Trigger.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/04_Cloud_Scheduler_Trigger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_WNxUbMmgfw"
      },
      "source": [
        "# Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9JhU_tzmgfz"
      },
      "source": [
        "1. Create Pub/Sub Topic ([refer](https://github.com/sayakpaul/CI-CD-for-Model-Training/blob/main/cloud_function_trigger.ipynb))\n",
        "2. Deploy Cloud Function ([refer](https://github.com/sayakpaul/CI-CD-for-Model-Training/blob/main/cloud_function_trigger.ipynb))\n",
        "    - check if there are enough number of images in a specific GCS bucket\n",
        "3. Publish Pub/Sub Topic to trigger batch prediction pipeline ([refer](https://github.com/sayakpaul/CI-CD-for-Model-Training/blob/main/cloud_scheduler_trigger.ipynb))\n",
        "    - need pipeline JSON spec somewhere in GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iva2o8C-mujw"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A7-ml0Yt6lX"
      },
      "source": [
        "!pip install --upgrade -q google-cloud-scheduler"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJLNWsPamwNw"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRkHpHnQmzof"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH4ZvM_3m397"
      },
      "source": [
        "GOOGLE_CLOUD_PROJECT = \"gcp-ml-172005\" #@param {type:\"string\"}\n",
        "GOOGLE_CLOUD_REGION = \"us-central1\"\n",
        "\n",
        "GCS_BUCKET_NAME = 'cifar10-experimental-csp2'    #@param {type:\"string\"}\n",
        "PIPELINE_NAME = \"continuous-adaptation-for-data-changes-batch\" #@param {type:\"string\"}\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME) \n",
        "PIPELINE_LOCATION = f\"{PIPELINE_ROOT}/{PIPELINE_NAME}.json\" \n",
        "\n",
        "PUBSUB_TOPIC = f\"trigger-{PIPELINE_NAME}\" \n",
        "\n",
        "SCHEDULER_JOB_NAME = f\"scheduler-job-{PUBSUB_TOPIC}\"\n",
        "\n",
        "IMAGE_LOCATION = 'gs://batch-prediction-collection-3' #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpidhm309Ed9",
        "outputId": "68f22276-fa64-45f9-a684-999b7ca298ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "IMAGE_LOCATION"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gs://batch-prediction-collection-3'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3622c4BaodLT"
      },
      "source": [
        "# Create Pub/Sub Topic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1RNMdR-ofBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53dac52-ab2b-49ad-d92c-eec4782fad1b"
      },
      "source": [
        "!gcloud pubsub topics create {PUBSUB_TOPIC}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created topic [projects/gcp-ml-172005/topics/trigger-continuous-adaptation-for-data-changes-batch].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2ISbBXvoiN7"
      },
      "source": [
        "# Deploy Cloud Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeYmsiYAqroy"
      },
      "source": [
        "### Create Cloud Function Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_hJOA47prkp"
      },
      "source": [
        "!mkdir -p cloud_function\n",
        "!touch cloud_function/__init__.py"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3gLM4SQqxMa"
      },
      "source": [
        "### Create Requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-7JN3huqdEO"
      },
      "source": [
        "_cloud_function_dep = 'cloud_function/requirements.txt'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9a9myWFqj14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758282b9-fd5c-4fa9-b83f-ac9965745a9d"
      },
      "source": [
        "%%writefile {_cloud_function_dep}\n",
        "\n",
        "kfp==1.6.2\n",
        "google-cloud-aiplatform\n",
        "google-cloud-storage"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cloud_function/requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhaDWKaRqzzH"
      },
      "source": [
        "### Create Cloud Function Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwldC4ntpD7Z"
      },
      "source": [
        "_cloud_function_file = 'cloud_function/main.py'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59_-cyfIonqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348f7f07-609b-4b01-9269-3bf640c83678"
      },
      "source": [
        "%%writefile {_cloud_function_file}\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import base64\n",
        "\n",
        "from kfp.v2.google.client import AIPlatformClient\n",
        "from google.cloud import storage\n",
        "\n",
        "def get_number_of_images(storage_client, gcs_image_file_location):\n",
        "    path_parts = gcs_image_file_location.replace(\"gs://\", \"\").split(\"/\")\n",
        "    bucket_name = path_parts[0]\n",
        "\n",
        "    blobs = storage_client.list_blobs(bucket_name)\n",
        "\n",
        "    count = 0\n",
        "    for blob in blobs:\n",
        "      if blob.name.split('.')[-1] == \"jpg\":\n",
        "        count = count+1\n",
        "\n",
        "    return count\n",
        "\n",
        "def is_there_enough_images(storage_client, gcs_image_file_location, threshold):\n",
        "    number_of_images = get_number_of_images(storage_client, gcs_image_file_location)\n",
        "    print(f'number of images = {number_of_images}')\n",
        "    return number_of_images >= threshold\n",
        "\n",
        "def trigger_pipeline(event, context):\n",
        "    # Parse the environment variables.\n",
        "    project = os.getenv(\"PROJECT\")\n",
        "    region = os.getenv(\"REGION\")\n",
        "    gcs_pipeline_file_location = os.getenv(\"GCS_PIPELINE_FILE_LOCATION\")\n",
        "    gcs_image_file_location = os.getenv(\"GCS_IMAGE_FILE_LOCATION\")\n",
        "\n",
        "    print(project)\n",
        "    print(region)\n",
        "    print(gcs_pipeline_file_location)\n",
        "    print(gcs_image_file_location)\n",
        "    \n",
        "    threshold = 100\n",
        "\n",
        "    # Check if the pipeline file exists in the provided GCS Bucket.\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    if is_there_enough_images(storage_client, gcs_image_file_location, threshold):\n",
        "      path_parts = gcs_pipeline_file_location.replace(\"gs://\", \"\").split(\"/\")\n",
        "      bucket_name = path_parts[0]\n",
        "      blob_name = \"/\".join(path_parts[1:])\n",
        "\n",
        "      bucket = storage_client.bucket(bucket_name)\n",
        "      blob = storage.Blob(bucket=bucket, name=blob_name)\n",
        "\n",
        "      if not blob.exists(storage_client):\n",
        "          raise ValueError(f\"{gcs_pipeline_file_location} does not exist.\")\n",
        "      \n",
        "      # Parse the data from the Pub/Sub trigger message.\n",
        "      # data = base64.b64decode(event[\"data\"]).decode(\"utf-8\")\n",
        "      # logging.info(f\"Event data: {data}\")\n",
        "\n",
        "      # parameter_values = json.loads(data)\n",
        "      \n",
        "      # Initialize Vertex AI API client and submit for pipeline execution.\n",
        "      api_client = AIPlatformClient(project_id=project, region=region)\n",
        "\n",
        "      response = api_client.create_run_from_job_spec(\n",
        "          job_spec_path=gcs_pipeline_file_location,\n",
        "          # parameter_values=parameter_values,\n",
        "          enable_caching=True,\n",
        "      )\n",
        "\n",
        "      logging.info(response)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting cloud_function/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUjuI5z7rH0Z"
      },
      "source": [
        "### Deploy Cloud Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zjMovG1WyIV"
      },
      "source": [
        "!cd cloud_function"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcMJL-9qpoP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f506aaa7-b7cf-40e5-91a7-57fe02def785"
      },
      "source": [
        "ENV_VARS=f\"\"\"\\\n",
        "PROJECT={GOOGLE_CLOUD_PROJECT},\\\n",
        "REGION={GOOGLE_CLOUD_REGION},\\\n",
        "GCS_PIPELINE_FILE_LOCATION={PIPELINE_LOCATION},\\\n",
        "GCS_IMAGE_FILE_LOCATION={IMAGE_LOCATION}\n",
        "\"\"\"\n",
        "\n",
        "!echo {ENV_VARS}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROJECT=gcp-ml-172005,REGION=us-central1,GCS_PIPELINE_FILE_LOCATION=gs://cifar10-experimental-csp2/pipeline_root/continuous-adaptation-for-data-changes-batch/continuous-adaptation-for-data-changes-batch.json,GCS_IMAGE_FILE_LOCATION=gs://batch-prediction-collection-3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdw7HUcSpBU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "738364ee-b93f-4f30-cc3d-7ed9c3c8c2e4"
      },
      "source": [
        "BUCKET = f'gs://{GCS_BUCKET_NAME}'\n",
        "CLOUD_FUNCTION_NAME = f'trigger-{PIPELINE_NAME}-fn'\n",
        "\n",
        "!gcloud functions deploy {CLOUD_FUNCTION_NAME} \\\n",
        "    --region={GOOGLE_CLOUD_REGION} \\\n",
        "    --trigger-topic={PUBSUB_TOPIC} \\\n",
        "    --runtime=python37 \\\n",
        "    --source=cloud_function\\\n",
        "    --entry-point=trigger_pipeline\\\n",
        "    --stage-bucket={BUCKET}\\\n",
        "    --update-env-vars={ENV_VARS}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For Cloud Build Logs, visit: https://console.cloud.google.com/cloud-build/builds;region=us-central1/dcc7b7aa-780f-423c-93a3-ff5abce7b5f0?project=874401645461\n",
            "availableMemoryMb: 256\n",
            "buildId: e7093ad3-2aa2-4d46-899b-844118c226ea\n",
            "buildName: projects/874401645461/locations/us-central1/builds/e7093ad3-2aa2-4d46-899b-844118c226ea\n",
            "entryPoint: trigger_pipeline\n",
            "environmentVariables:\n",
            "  GCS_IMAGE_FILE_LOCATION: gs://batch-prediction-collection-3\n",
            "  GCS_PIPELINE_FILE_LOCATION: gs://cifar10-experimental-csp2/pipeline_root/continuous-adaptation-for-data-changes-batch/continuous-adaptation-for-data-changes-batch.json\n",
            "  PROJECT: gcp-ml-172005\n",
            "  REGION: us-central1\n",
            "eventTrigger:\n",
            "  eventType: google.pubsub.topic.publish\n",
            "  failurePolicy: {}\n",
            "  resource: projects/gcp-ml-172005/topics/trigger-continuous-adaptation-for-data-changes-batch\n",
            "  service: pubsub.googleapis.com\n",
            "ingressSettings: ALLOW_ALL\n",
            "labels:\n",
            "  deployment-tool: cli-gcloud\n",
            "name: projects/gcp-ml-172005/locations/us-central1/functions/trigger-continuous-adaptation-for-data-changes-batch-fn\n",
            "runtime: python37\n",
            "serviceAccountEmail: gcp-ml-172005@appspot.gserviceaccount.com\n",
            "sourceArchiveUrl: gs://cifar10-experimental-csp2/us-central1-projects/gcp-ml-172005/locations/us-central1/functions/trigger-continuous-adaptation-for-data-changes-batch-fn-rmqssagatwoj.zip\n",
            "status: ACTIVE\n",
            "timeout: 60s\n",
            "updateTime: '2021-10-18T01:20:34.945Z'\n",
            "versionId: '9'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEZIpLjNrNe6"
      },
      "source": [
        "### See the Progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOudc6YvrPZA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c70e204-86c8-44f7-db60-60cf1985aa7b"
      },
      "source": [
        "import IPython\n",
        "\n",
        "cloud_fn_url = f\"https://console.cloud.google.com/functions/details/{GOOGLE_CLOUD_REGION}/{CLOUD_FUNCTION_NAME}\"\n",
        "html = (\n",
        "    f'See the Cloud Function details <a href=\"{cloud_fn_url}\" target=\"_blank\">here</a>.'\n",
        ")\n",
        "IPython.display.display(IPython.display.HTML(html))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Cloud Function details <a href=\"https://console.cloud.google.com/functions/details/us-central1/trigger-continuous-adaptation-for-data-changes-batch-fn\" target=\"_blank\">here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iknpM94_tnOc"
      },
      "source": [
        "# Create Cloud Scheduler's Job"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dx03Q6Qt0n4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a6f937-a4fc-4a60-859c-6fef96ae4df2"
      },
      "source": [
        "!gcloud scheduler jobs create pubsub $SCHEDULER_JOB_NAME --schedule \"*/3 * * * *\" --topic $PUBSUB_TOPIC --attributes name=scheduler #every hour"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: projects/gcp-ml-172005/locations/us-central1/jobs/scheduler-job-trigger-continuous-adaptation-for-data-changes-batch\n",
            "pubsubTarget:\n",
            "  attributes:\n",
            "    name: scheduler\n",
            "  topicName: projects/gcp-ml-172005/topics/trigger-continuous-adaptation-for-data-changes-batch\n",
            "retryConfig:\n",
            "  maxBackoffDuration: 3600s\n",
            "  maxDoublings: 16\n",
            "  maxRetryDuration: 0s\n",
            "  minBackoffDuration: 5s\n",
            "schedule: '*/3 * * * *'\n",
            "state: ENABLED\n",
            "timeZone: Etc/UTC\n",
            "userUpdateTime: '2021-10-18T01:10:04Z'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmpvGa-sZ012"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}