{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom_Model_TFX",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/Continuous-Adaptation-for-Machine-Learning-System-to-Data-Changes/blob/main/notebooks/Custom_Model_TFX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVp-9PGYFIO"
      },
      "source": [
        "This notebook assumes you are familiar with the basics of Vertex AI, TFX (especially custom components), and TensorFlow. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7gJqmqrsfqh"
      },
      "source": [
        "## References\n",
        "\n",
        "This notebook refers to the following resources and also reuses parts of the code from there: \n",
        "* [Simple TFX Pipeline for Vertex Pipelines](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/gcp/vertex_pipelines_simple.ipynb)\n",
        "* [Vertex AI Training with TFX and Vertex Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training)\n",
        "* [Importing models to Vertex AI](https://cloud.google.com/vertex-ai/docs/general/import-model)\n",
        "* [Deploying a model using the Vertex AI API](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)\n",
        "* [MLOPs with Vertex AI](https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai)\n",
        "* [Custom components TFX](https://www.tensorflow.org/tfx/tutorials/tfx/python_function_component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D04aKMGWXjOu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_niUhp_TY1G"
      },
      "source": [
        "# Use the latest version of pip.\n",
        "%%capture\n",
        "!pip install --upgrade tfx==1.2.0 kfp==1.6.1\n",
        "!pip install -q --upgrade google-cloud-aiplatform"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVmgQ6w1oT_Z"
      },
      "source": [
        "### ***Please restart runtime before continuing.*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mstgsNHWoiXk"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl8ewjX3oXRx"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqVWpmywXngD"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wptXF0e-UXsT",
        "outputId": "1bd4eb8b-943b-4d36-886a-f2cee7f9cd4e"
      },
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))\n",
        "import kfp\n",
        "print('KFP version: {}'.format(kfp.__version__))\n",
        "\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.5.1\n",
            "TFX version: 1.2.0\n",
            "KFP version: 1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFYHeepnXxpZ"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPVyBrXrW-vu"
      },
      "source": [
        "GOOGLE_CLOUD_PROJECT = 'central-hangar-321813'    #@param {type:\"string\"}\n",
        "GOOGLE_CLOUD_REGION = 'us-central1'             #@param {type:\"string\"}\n",
        "GCS_BUCKET_NAME = 'cifar10-experimental-csp'            #@param {type:\"string\"}\n",
        "\n",
        "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
        "    from absl import logging\n",
        "    logging.error('Please set all required parameters.')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV-BZSvQq7YY"
      },
      "source": [
        "The location of the bucket must be a single region. Also, the bucket needs to be created in a region when [Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J65KHrt4X-Fu",
        "outputId": "a0471caa-f240-4c78-ea9f-28fdfe65cf37"
      },
      "source": [
        "PIPELINE_NAME = 'continuous-adaptation-for-data-changes'\n",
        "\n",
        "# Path to various pipeline artifact.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# Paths for users' Python module.\n",
        "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# Paths for input data.\n",
        "DATA_ROOT = 'gs://cifar10-csp-public'\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PIPELINE_ROOT: gs://cifar10-experimental-csp/pipeline_root/continuous-adaptation-for-data-changes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQVpzyftX0y0"
      },
      "source": [
        "## Create training modules (Not Yet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGJU5sXrrAJW"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEbNM9CeERX2"
      },
      "source": [
        "# Specify training worker configurations. To minimize costs we can even specify two\n",
        "# different configurations: a beefier machine for the Endpoint model and slightly less\n",
        "# powerful machine for the mobile model.\n",
        "TRAINING_JOB_SPEC = {\n",
        "    'project': GOOGLE_CLOUD_PROJECT,\n",
        "    'worker_pool_specs': [{\n",
        "        'machine_spec': {\n",
        "            'machine_type': 'n1-standard-4',\n",
        "            'accelerator_type': 'NVIDIA_TESLA_K80',\n",
        "            'accelerator_count': 1\n",
        "        },\n",
        "        'replica_count': 1,\n",
        "        'container_spec': {\n",
        "            'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
        "        },\n",
        "    }],\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln1cvbcfphA9"
      },
      "source": [
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.components.example_gen import utils\n",
        "\n",
        "def _create_pipeline(\n",
        "    pipeline_name: str,\n",
        "    pipeline_root: str,\n",
        "    data_root: str,\n",
        "    project_id: str,\n",
        "    region: str,\n",
        ") -> tfx.dsl.Pipeline:\n",
        "    \"\"\"Creates a three component flowers pipeline with TFX.\"\"\"\n",
        "    splits = [\n",
        "      example_gen_pb2.Input.Split(name='train',pattern='span-{SPAN}/train/*'),\n",
        "      example_gen_pb2.Input.Split(name='val',pattern='span-{SPAN}/test/*')\n",
        "    ]\n",
        "    _, span, version = utils.calculate_splits_fingerprint_span_and_version(data_root, splits)\n",
        "\n",
        "    input_config = example_gen_pb2.Input(splits=[\n",
        "      example_gen_pb2.Input.Split(name='train', pattern=f'span-{span}/train/*'),\n",
        "                  example_gen_pb2.Input.Split(name='val', pattern=f'span-{span}/test/*')\n",
        "    ])\n",
        "    example_gen = tfx.components.ImportExampleGen(input_base=data_root,\n",
        "                                                  input_config=input_config)\n",
        "\n",
        "    components = [\n",
        "        example_gen,\n",
        "    ]\n",
        "\n",
        "    return tfx.dsl.Pipeline(\n",
        "        pipeline_name=pipeline_name, pipeline_root=pipeline_root, components=components\n",
        "    )\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFdlslfOX54z"
      },
      "source": [
        "## Compile the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AY5Z2tbsbwE"
      },
      "source": [
        "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
        "\n",
        "# Important: We need to pass the custom Docker image URI to the\n",
        "# `KubeflowV2DagRunnerConfig` to take effect.\n",
        "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
        "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(display_name=PIPELINE_NAME),\n",
        "    output_filename=PIPELINE_DEFINITION_FILE)\n",
        "\n",
        "_ = runner.run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        data_root=DATA_ROOT,\n",
        "        project_id=GOOGLE_CLOUD_PROJECT,\n",
        "        region=GOOGLE_CLOUD_REGION\n",
        "    )\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHBJaR_X7x2"
      },
      "source": [
        "## Submit the pipeline for execution to Vertex AI\n",
        "\n",
        "Generally, it's a good idea to first do a local run of the end-to-end pipeline before submitting it an online orchestrator. We can use `tfx.orchestration.LocalDagRunner()` for that but for the purposes of this notebook we won't be doing that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "3elrtDOus83z",
        "outputId": "7e062476-18ee-4bcf-a4e6-740b7421a0c7"
      },
      "source": [
        "from kfp.v2.google import client\n",
        "\n",
        "pipelines_client = client.AIPlatformClient(\n",
        "    project_id=GOOGLE_CLOUD_PROJECT,\n",
        "    region=GOOGLE_CLOUD_REGION,\n",
        ")\n",
        "\n",
        "_ = pipelines_client.create_run_from_job_spec(PIPELINE_DEFINITION_FILE, enable_caching=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/continuous-adaptation-for-data-changes-20210915042817?project=central-hangar-321813\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}